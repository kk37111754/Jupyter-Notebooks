{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6a9768d",
   "metadata": {},
   "source": [
    "## 深度学习是啥\n",
    "\n",
    "在人工智能领域，有一个方法叫机器学习。机器学习很难，但是到底有多难呢？[很难很难](https://www.shujudaka.com/documents/ji-qi-xue-xi-nan-zai-na.html)。\n",
    "\n",
    "在机器学习这个方法里，有一类算法叫神经网络。神经网络如下图所示：\n",
    "\n",
    "\n",
    "![eH7zHM](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/eH7zHM.jpg)\n",
    "\n",
    "上图中每个圆圈都是一个神经元，每条线表示神经元之间的连接。我们可以看到，上面的神经元被分成了多层，层与层之间的神经元有连接，而层内之间的神经元没有连接。最左边的层叫做**输入层**，这层负责接收输入数据；最右边的层叫**输出层**，我们可以从这层获取神经网络输出数据。输入层和输出层之间的层叫做**隐藏层**。\n",
    "\n",
    "隐藏层比较多（大于2）的神经网络叫做深度神经网络。而**深度学习，就是使用深层架构（比如，深度神经网络）的机器学习方法。**\n",
    "\n",
    "那么深层网络和浅层网络相比有什么优势呢？简单来说深层网络能够表达力更强。事实上，一个仅有一个隐藏层的神经网络就能拟合任何一个函数，但是它需要很多很多的神经元。而深层网络用少得多的神经元就能拟合同样的函数。也就是为了拟合一个函数，要么使用一个浅而宽的网络，要么使用一个深而窄的网络。而后者往往更节约资源。\n",
    "\n",
    "深层网络也有劣势，就是它不太容易训练。简单的说，你需要大量的数据，很多的技巧才能训练好一个深层网络。这是个手艺活。\n",
    "\n",
    "\n",
    "\n",
    "## 深度学习的近期历史\n",
    "\n",
    "[参考](https://developer.aliyun.com/article/72845?spm=5176.24320532.content1.1.4c091abdpUz93L)\n",
    "\n",
    "\n",
    "![GicYjD](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/GicYjD.jpg)\n",
    "\n",
    "AlexNet\n",
    "\n",
    "自2012年Deep Learning的代表模型AlexNet在[ImageNet大赛](https://www.shujudaka.com/documents/imagenet-li-nian-guan-jun-he-xiang-guan-cnn-mo-xin.html)中力压亚军，以超过10个百分点的绝对优势夺得头筹之后，依托于建模技术的进步、硬件计算能力的提升、优化技术的进步以及海量数据的累积，Deep Learning在语音、图像以及文本等多个领域不断推进，相较于传统作法取得了显著的效果提升。\n",
    "\n",
    " \n",
    "\n",
    "工业界和学术界也先后推出了用于Deep Learning建模用途的开源工具和框架，包括Caffe、Theano、Torch、MXNet、TensorFlow、Chainer、CNTK等等。其中MXNet、TensorFlow以及CNTK均对于训练过程提供了多机分布式支持，在相当大程度上解放了DL建模同学的生产力。\n",
    "\n",
    "但是，DL领域的建模技术突飞猛进，模型复杂度也不断增加。从模型的深度来看，以图像识别领域为例，12年的经典模型AlexNet由5个卷积层，3个全连接层构成（图1），在当时看来已经算是比较深的复杂模型，\n",
    "\n",
    "![CSxNlV](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/CSxNlV.jpg)\n",
    "图1. AlexNet模型示例\n",
    "\n",
    "而到了15年， 微软亚洲研究院则推出了由151个卷积层构成的极深网络ResNet（图2）；\n",
    "\n",
    "![1oWhlZ](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/1oWhlZ.jpg)\n",
    "图2. 36层的ResNet模型示例\n",
    "\n",
    "\n",
    "\n",
    "从模型的尺寸来看，在机器翻译领域，即便是仅仅由单层双向encoder，单层decoder构成的NMT模型（图3），在阿里巴巴的一个内部训练场景下，模型尺寸也达到了3GB左右的规模。\n",
    "\n",
    "![Fr2Znb](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/Fr2Znb.jpg)\n",
    "图3. NMT模型架构示例\n",
    "\n",
    "\n",
    "从模型的计算量来看，上面提到的机器翻译模型在单块M40 NVIDIA GPU上，完成一次完整训练，也需要耗时近三周。Deep Learning通过设计复杂模型，依托于海量数据的表征能力，从而获取相较于经典shallow模型更优的模型表现的建模策略对于底层训练工具提出了更高的要求。现有的开源工具，往往会在性能上、显存支持上、生态系统的完善性上存在不同层面的不足，在使用效率上对于普通的算法建模用户并不够友好。阿里云推出的PAI(Platform of Artificial Intelligence)产品则致力于通过系统与算法协同优化的方式，来有效解决Deep Learning训练工具的使用效率问题，目前PAI集成了TensorFlow、Caffe、MXNet这三款流行的Deep Learning框架，并针对这几款框架做了定制化的性能优化支持，以求更好的解决用户建模的效率问题。\n",
    "\n",
    " \n",
    "\n",
    "这些优化目前都已经应用在阿里巴巴内部的诸多业务场景里，包括黄图识别、OCR识别、机器翻译、智能问答等，这些业务场景下的某些建模场景会涉及到几十亿条规模的训练样本，数GB的模型尺寸，均可以在我们的优化策略下很好地得到支持和满足。经过内部大规模数据及模型场景的检测之后，我们也期望将这些能力输出，更好地赋能给阿里外部的AI从业人员。\n",
    "\n",
    "[性价比](https://zhuanlan.zhihu.com/p/61411536)\n",
    "\n",
    "![WRxxYR](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/WRxxYR.png)\n",
    "\n",
    "![HMrwW3](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/HMrwW3.jpg)\n",
    "\n",
    "![b24wIG](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/b24wIG.png)\n",
    "\n",
    "![68MxHG](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/68MxHG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce66ccc",
   "metadata": {},
   "source": [
    "## 感知器\n",
    "\n",
    "看到这里，如果你还是一头雾水，那也是很正常的。为了理解神经网络，我们应该先理解神经网络的组成单元——**神经元**。神经元也叫做**感知器**。感知器算法在上个世纪50-70年代很流行，也成功解决了很多问题。并且，感知器算法也是非常简单的。\n",
    "\n",
    "### 感知器的定义\n",
    "\n",
    "下图是一个感知器：\n",
    "\n",
    "![QJjHiw](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/QJjHiw.jpg)\n",
    "\n",
    "可以看到, 一个感知器有如下组成部分：\n",
    "- **输入权值** 一个感知器可以接收多个输入 $\\left(x_{1}, x_{2}, \\ldots, x_{n} \\mid x_{i} \\in \\mathbf{R}\\right)$, 每个输入上有一个权值 $w_{i} \\in \\mathbf{R}$, 此外还有一 个偏置项 $b \\in \\mathbf{R}$, 就是上图中的 $w_{0}$ 。\n",
    "- **激活函数** 感知器的激活函数可以有很多选择, 比如我们可以选择下面这个阶跃函数 $f$ 来作为激活函数:\n",
    "\n",
    "    阶跃函数是一种特殊的连续时间函数，是一个从0跳变到1的过程，属于奇异函数。在电路分析中，阶跃函数是研究动态电路阶跃响应的基础。利用阶跃函数可以进行信号处理、积分变换。在其他各个领域如自然生态、计算、工程等等均有不同程度的研究。\n",
    "\n",
    "![Dfk1zk](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/Dfk1zk.jpg)\n",
    "\n",
    "$$\n",
    "f(z)= \\begin{cases}1 & z>0 \\\\ 0 & \\text { otherwise }\\end{cases}\n",
    "$$\n",
    "- **输出** 感知器的输出由下面这个公式来计算\n",
    "$$\n",
    "y=f(\\mathrm{w} \\bullet \\mathrm{x}+b)\n",
    "$$\n",
    "\n",
    "\n",
    "### 使用感知器模拟and运算\n",
    "\n",
    "我们设计一个感知器，让它来实现and运算。程序员都知道，and是一个二元函数（带有两个参数和），下面是它的真值表：\n",
    "\n",
    "x1 | x2 | 输出结果\n",
    "-- | -- | ----\n",
    "0  | 0  | 0   \n",
    "0  | 1  | 0   \n",
    "1  | 0  | 0   \n",
    "1  | 1  | 1   \n",
    "\n",
    "则针对这个and 感知器的模型应该是下面这样子的：\n",
    "\n",
    "为了计算方便，我们用0表示false，用1表示true。这没什么难理解的，对于C语言程序员来说，这是天经地义的。\n",
    "\n",
    "我们令 $w_{1}=0.5 ; w_{2}=0.5 ; b=-0.8$, 而激活函数 $f$ 就是前面写出来的阶跃函数, 这时, 感知器就相当于 and 函数。 不明白? 我们验算一下:\n",
    "输入上面真值表的第一行, 即 $x_{1}=0 ; x_{2}=0$, 那么根据公式(1), 计算输出:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y &=f(w \\bullet x+b) \\\\\n",
    "&=f\\left(w_{1} x_{1}+w_{2} x_{2}+b\\right) \\\\\n",
    "&=f(0.5 \\times 0+0.5 \\times 0-0.8) \\\\\n",
    "&=f(-0.8) \\\\\n",
    "&=0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "也就是当$x_{1}$,$x_{2}$都为0的时候，$y$为0，这就是真值表的第一行。读者可以自行验证上述真值表的第二、三、四行。\n",
    "\n",
    "\n",
    "![ie5qLZ](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/ie5qLZ.jpg)\n",
    "\n",
    "\n",
    "1. b是偏置项，在对应的一组（x，y）下调整 b，每次调整多大？由此需要引入一个学习速率rate，调整的进度就是b = rate*误差\n",
    "\n",
    "2. 误差的计算在当前权值下计算的输出和理论输出的差值，如对于and 运算输入都为1时(x,y) = （(1,0),0）如果感知器的输出结果是1 则误差为0-1 。\n",
    "\n",
    "3. 权重的调整，输入* 学习速度 * 误差（weight = weight + x * rate * 误差）\n",
    "\n",
    "### 使用感知器实现`or`函数\n",
    "\n",
    "同样，我们也可以用感知器来实现`or`运算。仅仅需要把偏置项 b 的值设置为-0.3就可以了。我们验算一下，下面是`or`运算的**真值表**：\n",
    "\n",
    "x1 | x2 | 输出结果\n",
    "-- | -- | ----\n",
    "0  | 0  | 0   \n",
    "0  | 1  | 1   \n",
    "1  | 0  | 1   \n",
    "1  | 1  | 1\n",
    "\n",
    "我们来验算第二行, 这时的输入是 $x_{1}=0 ; x_{2}=1$, 带入公式(1):\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y &=f(\\mathrm{w} \\bullet \\mathrm{x}+b) \\\\\n",
    "&=f\\left(w_{1} x_{1}+w_{2} x_{2}+b\\right) \\\\\n",
    "&=f(0.5 \\times 1+0.5 \\times 0-0.3) \\\\\n",
    "&=f(0.2) \\\\\n",
    "&=1\n",
    "\\end{aligned}\n",
    "$$\n",
    "也就是当 $x_{1}=0 ; x_{2}=1$ 时， $y$ 为 1 , 即 or 真值表第二行。读者可以自行验证其它行。\n",
    "\n",
    "\n",
    "### 感知器的训练\n",
    "\n",
    "现在, 你可能困惑前面的权重项和偏置项的值是如何获得的呢? 这就要用到感知器训练算法: 将权重项和偏置项初始化为 0 , 然后, 利用下面的感知器规则迭代的修改 $w_{i}$ 和 $b$, 直到训练完成。\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w_{i} & \\leftarrow w_{i}+\\Delta w_{i} \\\\\n",
    "b & \\leftarrow b+\\Delta b\n",
    "\\end{aligned}\n",
    "$$\n",
    "其中:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Delta w_{i} &=\\eta(t-y) x_{i} \\\\\n",
    "\\Delta b &=\\eta(t-y)\n",
    "\\end{aligned}\n",
    "$$\n",
    "$w_{i}$ 是与输入 $x_{i}$ 对应的权重项, $b$ 是偏置项。事实上, 可以把 $b$ 看作是值永远为 1 的输入 $x_{b}$ 所对应的权重。 $t$ 是训练样本的实 际值, 一般称之为label。而 $y$ 是感知器的输出值, 它是根据公式(1)计算得出。 $\\eta$ 是一个称为学习速率的常数, 其作用是控 制每一步调整权的幅度。\n",
    "每次从训练数据中取出一个样本的输入向量 $\\mathrm{x}$, 使用感知器计算其输出 $y$, 再根据上面的规则来调整权重。每处理一个样 本就调整一次权重。经过多轮迭代后（即全部的训练数据被反复处理多轮）, 就可以训练出感知器的权重, 使之实现目标 函数。\n",
    "\n",
    "\n",
    "### and,or 感知器实现\n",
    "\n",
    "\n",
    "https://github.com/WangLaoShi/PythonUtils/tree/main/MLDL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a7f632",
   "metadata": {},
   "source": [
    "## 感知器升华一下\n",
    "\n",
    "感知机（perceptron）：是一个二类分类的线性判断模型，其输入为实例的特征向量，输出为实例的类别，取`+1`和`–1`值，属于判别模型\n",
    "\n",
    "注：`+1 -1 分别代表正负类，有的可能用 1 0 表示`\n",
    "\n",
    "在介绍感知机定义之前，下面几个概念需要说明一下：\n",
    "\n",
    "- 输入空间：输入所有可能取值的集合\n",
    "- 输出空间：输出所有可能取值的集合\n",
    "- 特征空间：每个具体的输入是一个实例，由特征向量表示\n",
    "\n",
    "所以对于一个感知机模型，可以这样表示：\n",
    "\n",
    "- 输入空间（特征空间）：$\\chi \\subseteq \\mathbb{R} ^n$\n",
    "- 输出空间：$\\gamma = \\{+1,-1 \\}$\n",
    "\n",
    "那么感知机就是由输入空间到输出空间的函数：\n",
    "\n",
    "$$\\displaystyle f( x) \\ =\\ sign( w\\cdot x+b)$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $sign$: 符号函数\n",
    "- $w$: 权值（weight）或权值向量（weight vector）\n",
    "- $b$: 偏置（bias）\n",
    "\n",
    "感知机的几何解释如下：线性方程\n",
    "\n",
    "$$w\\cdot x + b =0$$\n",
    "\n",
    "如果是二维空间，感知机就是一个线性函数，将正负样本一分为二，如何是三维空间，那么感知机就是一个平面将类别一切为二，上升到n维空间的话，其对应的是特征空间$\\mathbb{R} ^n$的一个[超平面](https://zh.wikipedia.org/wiki/%E8%B6%85%E5%B9%B3%E9%9D%A2)$S$：\n",
    "\n",
    "[一个小哥讲的超平面](https://www.bilibili.com/video/BV1t7411R7qu/)\n",
    "\n",
    "[一个巨好的动画效果](https://www.bilibili.com/video/BV1Vt411Z7mG/)\n",
    "\n",
    "- $w$: 超平面的法向量\n",
    "- $b$: 超平面的截距\n",
    "\n",
    "### 感知机学习策略\n",
    "\n",
    "#### 数据集的线性可分性\n",
    "\n",
    "什么是数据集的线性可分性，很简单，对于一个数据集：\n",
    "\n",
    "$$T = \\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\}$$\n",
    "\n",
    "如果存在上面一节说的超平面$S$：$w\\cdot x + b =0$，能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，则称数据集T为线性可分数据集（linearly separable data set），否则，称数据集T线性不可分\n",
    "\n",
    "#### 感知机学习策略\n",
    "\n",
    "找出超平面$S$，其实就是确定感知机模型参数：$w b$，根据统计学习方法三要素(统计学习方法由三要素构成，可以简单地表示为：方法 = 模型 + 策略 + 算法)，此时我们需要确定一个学习策略，比如前面所说的损失函数（经验函数），并使其最小化（猜也猜得到策略讲完，后面就是说学习算法了哈哈）\n",
    "\n",
    "以线性代数为例子，用损失函数来度量预测错误的程度，这里的损失函数可以用误分类点到超平面$S$的总距离，输入空间$\\mathbb{R} ^n$中任一点$x_0$到超平面$S$的距离：\n",
    "\n",
    "$$\\frac{1}{||w||}|w\\cdot x_0+b|$$\n",
    "\n",
    "![rthfhj](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/rthfhj.png)\n",
    "\n",
    "![XqOADK](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/XqOADK.png)\n",
    "\n",
    "[一个讲范数很好的视频-试试自己能不能留学](https://www.youtube.com/watch?v=pgJ2Sg1jcYQ)\n",
    "\n",
    "[看看国内外关于技术方面的差别-中国篇](https://blog.csdn.net/a493823882/article/details/80569888)\n",
    "\n",
    "[看看国内外关于技术方面的差别-外国篇](https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.5-Norms/)\n",
    "\n",
    "其中，$||w||$是$w$的$L_2$范数，假设超平面S的误分类点集合为$M$，那么所有误分类点到超平面$S$的总距离为：\n",
    "\n",
    "$$-\\frac{1}{||w||}\\sum_{x_i\\in M} y_i(w\\cdot x_i + b)$$\n",
    "\n",
    "最终推导出感知机学习的损失函数：\n",
    "\n",
    "$$L(w,b) =-\\sum_{x_i\\in M} y_i(w\\cdot x_i + b)$$\n",
    "\n",
    "### 感知机学习算法\n",
    "\n",
    "上面一节已经确定了学习策略，按照统计学习方法三要素，目前需要一个算法来求解，目前最优化的方法是随机梯度下降法\n",
    "\n",
    "#### 感知机学习算法的原始形式\n",
    "\n",
    "现在感知机学习算法就是对下面最优化问题的算法：\n",
    "\n",
    "$$ \\min_{w,b} L(w,b) =-\\sum_{x_i\\in M} y_i(w\\cdot x_i + b) $$\n",
    "\n",
    "现在的问题就转化成，求出参数$w$和$b$，使得上列损失函数达到极小化，这里我直接贴出书中的算法，后面的例子我会用`Python`代码实现：\n",
    "\n",
    "![903ZUI](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/903ZUI.jpg)\n",
    "\n",
    "有了解题方法怎么能没有题目呢？李杭老师自然是考虑到了，请听题：\n",
    "\n",
    "![Pk3E7w](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/Pk3E7w.jpg)\n",
    "\n",
    "代码：https://github.com/WangLaoShi/PythonUtils/tree/main/MLDL\n",
    "\n",
    "代码写完了，再看看推导过程：\n",
    "\n",
    "![8h1PGq](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/8h1PGq.jpg)\n",
    "\n",
    "#### 算法的收敛性\n",
    "\n",
    "对于线性可分数据集感知机学习算法原始形式收敛，即经过有限次迭代可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型，定理`2.1`如下：\n",
    "\n",
    "假设训练数据集$T = \\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\\}$是线性可分的，其中$x_i\\in \\chi  =\\mathbb{R} ^n$，$y_i \\in \\gamma =\\{-1, 1\\}$，$i=1,2,...,N$，则有：\n",
    "\n",
    "![kyfzoC](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/kyfzoC.jpg)\n",
    "\n",
    "\n",
    "#### 感知机学习算法的对偶形式\n",
    "\n",
    "为什么要介绍感知机学习算法的对偶形式，主要目的就是减少运算量。\n",
    "\n",
    "![Wv6NXM](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/Wv6NXM.jpg)\n",
    "\n",
    "\n",
    "\n",
    "代码：https://github.com/WangLaoShi/PythonUtils/tree/main/MLDL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbd1bfa",
   "metadata": {},
   "source": [
    "## 另一个感知器，线性单元\n",
    "\n",
    "通过介绍另外一种『感知器』，也就是『线性单元』，来说明关于机器学习一些基本的概念，比如模型、目标函数、优化算法等等。这些概念对于所有的机器学习算法来说都是通用的，掌握了这些概念，就掌握了机器学习的基本套路。\n",
    "\n",
    "感知器有一个问题，当面对的数据集不是**线性可分**的时候，『感知器规则』可能无法收敛，这意味着我们永远也无法完成一个感知器的训练。为了解决这个问题，我们使用一个**可导**的**线性函数**来替代感知器的**阶跃函数**，这种感知器就叫做**线性单元**。线性单元在面对线性不可分的数据集时，会收敛到一个最佳的近似上。\n",
    "\n",
    "为了简单起见，我们可以设置线性单元的激活函数$f$为\n",
    "\n",
    "$$ f(x) = x $$\n",
    "\n",
    "![VAhs8U](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/VAhs8U.jpg)\n",
    "\n",
    "对比此前我们讲过的感知器\n",
    "\n",
    "![oIbCMF](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/oIbCMF.jpg)\n",
    "\n",
    "这样替换了激活函数$f$之后，**线性单元**将返回一个**实数值**而不是**0,1分类**。因此线性单元用来解决**回归**问题而不是**分类**问题。\n",
    "\n",
    "#### 线性单元的模型\n",
    "\n",
    "当我们说模型时, 我们实际上在谈论根据输入 $x$ 预测输出 $y$ 的算法。比如, $x$ 可以是一个人的工作年限, $y$ 可以是他的月 薪, 我们可以用某种算法来根据一个人的工作年限来预测他的收入。比如：\n",
    "\n",
    "$$\n",
    "y=h(x)=w * x+b\n",
    "$$\n",
    "\n",
    "函数 $h(x)$ 叫做假设, 而 $w 、 b$ 是它的参数。我们假设参数 $w=1000$, 参数 $b=500$, 如果一个人的工作年限是 5 年的话, 我们的模型会预测他的月薪为\n",
    "\n",
    "$$\n",
    "y=h(x)=1000 * 5+500=5500(\\text { 元 })\n",
    "$$\n",
    "\n",
    "你也许会说, 这个模型太不靠谱了。是这样的, 因为我们考虑的因素太少了, 仅仅包含了工作年限。如果考虑更多的因素, 比如所处的行业、公司、职级等等, 可能预测就会靠谱的多。\n",
    "\n",
    "我们把工作年限、行业、公司、职级这些信息, 称之为**特征**。\n",
    "\n",
    "对于一个工作了5年, 在IT行业, 百度工作, 职级T6这样的人, 我们可以用这样的一个特征向量来表示他 $\\mathrm{x}=(5, I T$, 百度, $T 6)$ 。\n",
    "\n",
    "既然输入 $x$ 变成了一个具备四个特征的向量, 相对应的, 仅仅一个参数 $w$ 就不够用了, 我们应该使用 4 个参数 $w_{1}, w_{2}, w_{3}, w_{4}$, 每个特征对应一个。这样, 我们的模型就变成\n",
    "\n",
    "$$\n",
    "y=h(x)=w_{1} * x_{1}+w_{2} * x_{2}+w_{3} * x_{3}+w_{4} * x_{4}+b\n",
    "$$\n",
    "\n",
    "其中, $x_{1}$ 对应工作年限, $x_{2}$ 对应行业, $x_{3}$ 对应公司, $x_{4}$ 对应职级。\n",
    "\n",
    "为了书写和计算方便, 我们可以令 $w_{0}$ 等于 $b$, 同时令 $w_{0}$ 对应于特征 $x_{0}$ 。由于 $x_{0}$ 其实并不存在, 我们可以令它的值永远为 1。也就是说\n",
    "\n",
    "$$\n",
    "b=w_{0} * x_{0} \\quad \\text { 其中 } x_{0}=1\n",
    "$$\n",
    "\n",
    "这样上面的式子就可以写成\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y=h(x) &=w_{1} * x_{1}+w_{2} * x_{2}+w_{3} * x_{3}+w_{4} * x_{4}+b \\\\\n",
    "&=w_{0} * x_{0}+w_{1} * x_{1}+w_{2} * x_{2}+w_{3} * x_{3}+w_{4} * x_{4}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "我们还可以把上式写成向量的形式\n",
    "\n",
    "$$\n",
    "y=h(x)=\\mathrm{w}^{T} \\mathbf{x} \n",
    "$$\n",
    "\n",
    "长成这种样子模型就叫做**线性模型**, 因为输出 $y$ 就是输入特征 $x_{1}, x_{2}, x_{3}, \\ldots$ 的**线性组合**。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5d057e",
   "metadata": {},
   "source": [
    "#### 监督学习和无监督学习\n",
    "\n",
    "接下来, 我们需要关心的是这个模型如何训练, 也就是参数w取什么值最合适。\n",
    "\n",
    "\n",
    "机器学习有一类学习方法叫做**监督学习**, 它是说为了训练一个模型, 我们要提供这样一堆训练样本: 每个训练样本既包括 输入特征 $\\mathrm{x}$, 也包括对应的输出 $y$ ( $y$ 也叫做标记, label)。\n",
    "\n",
    "也就是说, 我们要找到很多人, 我们既知道他们的特征(工作年 限, 行业...), 也知道他们的收入。\n",
    "\n",
    "我们用这样的样本去训练模型, 让模型既看到我们提出的每个问题(输入特征 x), 也看到对应问题的答案(标记 $y$ )。\n",
    "\n",
    "当模型看到足够多的样本之后, 它就能总结出其中的一些规律。然后, 就可以预测那些它没看过的输入所对应的答案了。\n",
    "\n",
    "\n",
    "另外一类学习方法叫做**无监督学习**, 这种方法的训练样本中只有 $x$ 而没有 $y$ 。模型可以总结出特征 $x$ 的一些规律, 但是无法知道其对应的答案 $y$ 。\n",
    "\n",
    "很多时候, 既有 $\\mathrm{x}$ 又有 $y$ 的训练样本是很少的, 大部分样本都只有 $\\mathrm{x}$ 。\n",
    "\n",
    "比如在**语音到文本(STT)**的识别任务中, $\\mathrm{x}$ 是语音, $y$ 是这段语音对应的文本。我们很容易获取大量的语音录音, 然而把语音一段一段切分好并标注上对应文字则是非常费力气的事情。这种情况下, 为了弥补带标注样本的不足, 我们可以用无监督学习方法先做一些聚类, 让模型总结出哪些音节是相似的, 然后再用少量的带标注的训练样本, 告诉模型其中一些音节对应的文字。这样模型就可以把相似的音节都对应到 相应文字上, 完成模型的训练。\n",
    "\n",
    "\n",
    "#### 线性单元的目标函数\n",
    "\n",
    "现在, 让我们只考虑监督学习。\n",
    "\n",
    "在监督学习下, 对于一个样本, 我们知道它的特征 $\\mathrm{x}$, 以及标记 $y$ 。同时, 我们还可以根据模型 $h(x)$ 计算得到输出 $\\bar{y}$ 。注意这里面我们用 $y$ 表示训练样本里面的标记, 也就是实际值; 用带上划线的 $\\bar{y}$ 表示模型计算的出来的预测值。我们当然希 望模型计算出来的 $\\bar{y}$ 和 $y$ 越接近越好。\n",
    "数学上有很多方法来表示的 $\\bar{y}$ 和 $y$ 的接近程度, 比如我们可以用 $\\bar{y}$ 和 $y$ 的差的平方的 $\\frac{1}{2}$ 来表示它们的接近程度（这个地方也要不得不提李航老师）\n",
    "\n",
    "$$\n",
    "e=\\frac{1}{2}(y-\\bar{y})^{2}\n",
    "$$\n",
    "\n",
    "我们把 $e$ 叫做单个样本的误差。\n",
    "\n",
    "至于为什么前面要乘 $\\frac{1}{2}$, 是为了后面计算方便。\n",
    "\n",
    "训练数据中会有很多样本, 比如 $N$ 个, 我们可以用训练数据中所有样本的误差的和, 来表示模型的误差 $E$, 也就是\n",
    "\n",
    "$$\n",
    "E=e^{(1)}+e^{(2)}+e^{(3)}+\\ldots+e^{(n)}\n",
    "$$\n",
    "\n",
    "我们还可以把上面的式子写成和式的形式。使用和式, 不光书写起来简单, 逼格也跟着暴涨, 一举两得。所以一定要写成\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E &=e^{(1)}+e^{(2)}+e^{(3)}+\\ldots \\\\\n",
    "&=\\sum_{i=1}^{n} e^{(i)} \\\\\n",
    "&=\\frac{1}{2} \\sum_{i=1}^{n}\\left(y^{(i)}-\\bar{y}^{(i)}\\right)^{2}  \\text{(式2)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "其中\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\bar{y}^{(i)} &=h\\left(\\mathrm{x}^{(i)}\\right) \\\\\n",
    "&=\\mathrm{w}^{T} \\mathrm{x}^{(\\mathrm{i})}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "式2中, $x^{(i)}$ 表示第 $i$ 个训练样本的**特征**, $y^{(i)}$ 表示第 $i$ 个样本的**标记**, 我们也可以用**元组** $\\left(x^{(i)}, y^{(i)}\\right)$ 表示第 $i$ **训练样本**。 $\\bar{y}^{(i)}$ 则是模型对第 $i$ 个样本的**预测值**。\n",
    "\n",
    "我们当然希望对于一个训练数据集来说, 误差最小越好, 也就是(式2)的值越小越好。对于特定的训练数据集来说,\n",
    "$\\left(x^{(i)}, y^{(i)}\\right)$ 的值都是已知的, 所以(式 2$)$ 其实是参数 $\\mathrm{w}$ 的函数。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E(\\mathrm{w}) &=\\frac{1}{2} \\sum_{i=1}^{n}\\left(y^{(i)}-\\bar{y}^{(i)}\\right)^{2} \\\\\n",
    "&=\\frac{1}{2} \\sum_{i=1}^{n}\\left(\\mathrm{y}^{(\\mathrm{i})}-\\mathrm{w}^{\\mathrm{T}} \\mathrm{x}^{(\\mathrm{i})}\\right)^{2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "由此可见, 模型的训练, 实际上就是求取到合适的 $\\mathrm{w}$, 使(式2)取得最小值。这在数学上称作**优化问题**, 而 $E(\\mathrm{w})$ 就是我们优化的目标, 称之为**目标函数**。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de2ab8d",
   "metadata": {},
   "source": [
    "#### 梯度下降优化算法\n",
    "\n",
    "大学时我们学过怎样求函数的极值。函数 $y=f(x)$ 的极值点, 就是它的导数 $f^{\\prime}(x)=0$ 的那个点。\n",
    "\n",
    "因此我们可以通过解 方程 $f^{\\prime}(x)=0$, 求得函数的极值点 $\\left(x_{0}, y_{0}\\right)$ 。\n",
    "\n",
    "不过对于计算机来说, 它可不会解方程。但是它可以任借强大的计算能力, 一步一步的去把函数的极值点『试』出来。如 下图所示:\n",
    "\n",
    "![2b6X2H](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/2b6X2H.jpg)\n",
    "\n",
    "首先, 我们随便选择一个点开始, 比如上图的 $x_{0}$ 点。接下来, 每次迭代修改 $x$ 的为 $x_{1}, x_{2}, x_{3}, \\ldots$, 经过数次迭代后最终 达到函数最小值点。\n",
    "\n",
    "你可能要问了, 为啥每次修改 $x$ 的值, 都能往函数最小值那个方向前进呢? 这里的奥秘在于, 我们每次都是向函数 $y=f(x)$ 的梯度的相反方向来修改 $x$ 。\n",
    "\n",
    "**什么是梯度呢?**\n",
    "\n",
    "[Youtube 梯度](https://www.youtube.com/watch?v=npkl19rcpdY)\n",
    "\n",
    "翻开大学高数课的课本, 我们会发现梯度是一个向量, 它指向函数值上升最快的方向。\n",
    "\n",
    "显然, 梯度的反方向当然就是函数值下降最快的方向了。\n",
    "\n",
    "我们每次沿着梯度相反方向去修改 $x$ 的值, 当然就能走到函数的最小值附近。之所以是最小值附近而不是最小值那个点, 是因为我们每次移动的步长不会那么恰到好处, 有可能最后一次迭代走远了越过了最小值那个点。\n",
    "\n",
    "步长的选择是门手艺, 如果选择小了, 那么就会迭代很多轮才 能走到最小值附近; 如果选择大了, 那可能就会越过最小值很远, 收敛不到一个好的点上。\n",
    "\n",
    "\n",
    "按照上面的讨论, 我们就可以写出**梯度下降算法**的公式\n",
    "\n",
    "$$\n",
    "\\mathrm{x}_{n e w}=\\mathrm{x}_{o l d}-\\eta \\nabla f(x)\n",
    "$$\n",
    "\n",
    "其中, $\\nabla$ 是梯度算子, $\\nabla f(x)$ 就是指 $f(x)$ 的梯度。 $\\eta$ 是步长, 也称作**学习速率**。\n",
    "\n",
    "对于上一节列出的目标函数(式 2)\n",
    "\n",
    "$$\n",
    "E(\\mathrm{w})=\\frac{1}{2} \\sum_{i=1}^{n}\\left(\\mathrm{y}^{(\\mathrm{i})}-\\overline{\\mathrm{y}}^{(\\mathrm{i})}\\right)^{2}\n",
    "$$\n",
    "\n",
    "梯度下降算法可以写成\n",
    "\n",
    "$$\n",
    "\\mathrm{w}_{n e w}=\\mathrm{w}_{\\text {old }}-\\eta \\nabla E(\\mathrm{w})\n",
    "$$\n",
    "\n",
    "聪明的你应该能想到，如果要求目标函数的**最大值**，那么我们就应该用**梯度上升**算法，它的参数修改规则是\n",
    "\n",
    "$$\n",
    "\\mathrm{w}_{n e w}=\\mathrm{w}_{\\text {old }}+\\eta \\nabla E(\\mathrm{w})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5e9670",
   "metadata": {},
   "source": [
    "下面, 请先做几次深呼吸, 让你的大脑补充足够的新鲜的氧气, 我们要来求取 $\\nabla E(\\mathrm{w})$, 然后带入上式, 就能得到线性单元的参数修改规则。\n",
    "\n",
    "\n",
    "关于 $\\nabla E(\\mathrm{w})$ 的推导过程, 我单独把它们放到一节中。您既可以选择慢慢看, 也可以选择无视。在这里, 您只需要知道, 经过一大串推导, 目标函数 $E(w)$ 的梯度是\n",
    "\n",
    "$$\n",
    "\\nabla E(\\mathrm{w})=-\\sum_{i=1}^{n}\\left(y^{(i)}-\\bar{y}^{(i)}\\right) \\mathrm{x}^{(i)}\n",
    "$$\n",
    "\n",
    "因此，线性单元的参数修改规则最后是这个样子\n",
    "\n",
    "$$\n",
    "\\mathrm{w}_{\\text {new }}=\\mathrm{w}_{\\text {old }}+\\eta \\sum_{i=1}^{n}\\left(y^{(i)}-\\bar{y}^{(i)}\\right) \\mathbf{x}^{(i)} \\text{(式 3)}\n",
    "$$\n",
    "\n",
    "有了上面这个式子，我们就可以根据它来写出训练线性单元的代码了。\n",
    "\n",
    "\n",
    "需要说明的是, 如果每个样本有 $M$ 个特征, 则上式中的 $x, w$ 都是 $M+1$ 维**向量**(因为我们加上了一个恒为 1 的虚拟特征 $x_{0}$,参考前面的内容)，而 $y$ 是标量。用高逼格的数学符号表示，就是\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\mathrm{x}, \\mathrm{w} \\in \\mathfrak{R}^{(M+1)} \\\\\n",
    "y \\in \\Re^{1}\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "\n",
    "为了让您看明白说的是啥, 我吐血写下下面这个解释。因为 $w, x$ 是 $M+1$ 维列向量, 所以(式3)可以\n",
    "\n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{c}\n",
    "w_{0} \\\\\n",
    "w_{1} \\\\\n",
    "w_{2} \\\\\n",
    "\\ldots \\\\\n",
    "w_{m}\n",
    "\\end{array}\\right]_{\\text {new }}=\\left[\\begin{array}{c}\n",
    "w_{0} \\\\\n",
    "w_{1} \\\\\n",
    "w_{2} \\\\\n",
    "\\ldots \\\\\n",
    "w_{m}\n",
    "\\end{array}\\right]_{\\text {old }}+\\eta \\sum_{i=1}^{n}\\left(y^{(i)}-\\bar{y}^{(i)}\\right)\\left[\\begin{array}{c}\n",
    "1 \\\\\n",
    "x_{1}^{(i)} \\\\\n",
    "x_{2}^{(i)} \\\\\n",
    "\\cdots \\\\\n",
    "x_{m}^{(i)}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "如果还是没看明白, 建议您也吐血再看一下大学时学过的《线性代数》吧。\n",
    "\n",
    "\n",
    "#### $\\nabla E(\\mathrm{w})$ 的推导\n",
    "\n",
    "\n",
    "这一节你尽可以跳过它, 并不太会影响到全文的理解。当然如果你非要弄明白每个细节, 那恭喜你骚年, 机器学习的末来 一定是属于你的。\n",
    "\n",
    "首先, 我们先做一个简单的前戏。我们知道函数的梯度的定义就是它相对于各个变量的**偏导数**, 所以我们写下下面的式子\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla E(\\mathrm{w}) &=\\frac{\\partial}{\\partial \\mathrm{w}} E(\\mathrm{w}) \\\\\n",
    "&=\\frac{\\partial}{\\partial \\mathrm{w}} \\frac{1}{2} \\sum_{i=1}^{n}\\left(y^{(i)}-\\bar{y}^{(i)}\\right)^{2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "可接下来怎么办呢? 我们知道和的导数等于导数的和, 所以我们可以先把求和符号 $\\sum$ 里面的导数求出来, 然后再把它们 加在一起就行了, 也就是\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\frac{\\partial}{\\partial \\mathrm{w}} \\frac{1}{2} \\sum_{i=1}^{n}\\left(y^{(i)}-\\bar{y}^{(i)}\\right)^{2} \\\\\n",
    "=& \\frac{1}{2} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\mathrm{w}}\\left(y^{(i)}-\\bar{y}^{(i)}\\right)^{2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "现在我们可以不管高大上的 $\\sum 了$, 先专心把里面的导数求出来。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\frac{\\partial}{\\partial \\mathrm{w}}\\left(y^{(i)}-\\bar{y}^{(i)}\\right)^{2} \\\\\n",
    "=& \\frac{\\partial}{\\partial \\mathrm{w}}\\left(y^{(i) 2}-2 \\bar{y}^{(i)} y^{(i)}+\\bar{y}^{(i) 2}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "我们知道, $y$ 是与 $\\mathrm{w}$ 无关的常数, 而 $\\bar{y}=\\mathrm{w}^{T} \\mathrm{x}$, 下面我们根据链式求导法则来求导(上大学时好像叫复合函数求导法则)\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E(\\mathrm{w})}{\\partial \\mathrm{w}}=\\frac{\\partial E(\\bar{y})}{\\partial \\bar{y}} \\frac{\\partial \\bar{y}}{\\partial \\mathrm{w}}\n",
    "$$\n",
    "\n",
    "我们分别计算上式等号右边的两个偏导数\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial E(\\mathrm{w})}{\\partial \\bar{y}} &=\\frac{\\partial}{\\partial \\bar{y}}\\left(y^{(i) 2}-2 \\bar{y}^{(i)} y^{(i)}+\\bar{y}^{(i) 2}\\right) \\\\\n",
    "&=-2 y^{(i)}+2 \\bar{y}^{(i)} \\\\\n",
    "\\frac{\\partial \\bar{y}}{\\partial \\mathrm{w}} &=\\frac{\\partial}{\\partial \\mathrm{w}} \\mathrm{w}^{T} \\mathrm{x} \\\\\n",
    "&=\\mathrm{x}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "代入, 我们求得 $\\sum$ 里面的偏导数是\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\frac{\\partial}{\\partial \\mathrm{w}}\\left(y^{(i)}-\\bar{y}^{(i)}\\right)^{2} \\\\\n",
    "=& 2\\left(-y^{(i)}+\\bar{y}^{(i)}\\right) \\mathrm{x}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "最后代入 $\\nabla E(\\mathrm{w})$, 求得\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla E(\\mathrm{w}) &=\\frac{1}{2} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\mathrm{w}}\\left(y^{(i)}-\\bar{y}^{(i)}\\right)^{2} \\\\\n",
    "&=\\frac{1}{2} \\sum_{i=1}^{n} 2\\left(-y^{(i)}+\\bar{y}^{(i)}\\right) \\mathrm{x} \\\\\n",
    "&=-\\sum_{i=1}^{n}\\left(y^{(i)}-\\bar{y}^{(i)}\\right) \\mathrm{x}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "至此, 大功告成。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2213dc77",
   "metadata": {},
   "source": [
    "#### 随机梯度下降算法(Stochastic Gradient Descent, SGD)\n",
    "\n",
    "\n",
    "如果我们根据(式3)来训练模型，那么我们每次更新的迭代，要遍历训练数据中所有的样本进行计算，我们称这种算法叫做批梯度下降(Batch Gradient Descent)。\n",
    "\n",
    "**如果我们的样本非常大，比如数百万到数亿，那么计算量异常巨大。**\n",
    "\n",
    "因此，实用的算法是SGD算法。在SGD算法中，每次更新的迭代，只计算一个样本。这样对于一个具有数百万样本的训练数据，完成一次遍历就会对更新数百万次，效率大大提升。\n",
    "\n",
    "由于样本的噪音和随机性，每次更新并不一定按照减少的方向。然而，虽然存在一定随机性，大量的更新总体上沿着减少的方向前进的，因此最后也能收敛到最小值附近。下图展示了SGD和BGD的区别\n",
    "\n",
    "![VpSNtZ](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/VpSNtZ.jpg)\n",
    "\n",
    "如上图，椭圆表示的是函数值的等高线，椭圆中心是函数的最小值点。红色是BGD的逼近曲线，而紫色是SGD的逼近曲线。我们可以看到BGD是一直向着最低点前进的，而SGD明显躁动了许多，但总体上仍然是向最低点逼近的。\n",
    "\n",
    "最后需要说明的是，SGD不仅仅效率高，而且随机性有时候反而是好事。今天的目标函数是一个『凸函数』，沿着梯度反方向就能找到全局唯一的最小值。然而对于非凸函数来说，存在许多局部最小值。随机性有助于我们逃离某些很糟糕的局部最小值，从而获得一个更好的模型。\n",
    "\n",
    "![kF3Arq](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/kF3Arq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edd8e4c",
   "metadata": {},
   "source": [
    "#### 实现线性单元\n",
    "\n",
    "因为我们已经写了感知器的代码，因此我们先比较一下感知器模型和线性单元模型，看看哪些代码能够复用。\n",
    "\n",
    "![2KxMX4](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/2KxMX4.png)\n",
    "\n",
    "代码：\n",
    "\n",
    "\n",
    "\n",
    "程序执行，拟合的直线\n",
    "\n",
    "![s5dAyZ](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/s5dAyZ.jpg)\n",
    "\n",
    "\n",
    "\n",
    "## 小结\n",
    "\n",
    "事实上，一个机器学习算法其实只有两部分\n",
    "\n",
    "* _模型_  从输入特征$x$ 预测输出 $y$的那个函数$h(x)$\n",
    "* _目标函数_ 目标函数取最小(最大)值时所对应的参数值，就是模型的参数的**最优值**。很多时候我们只能获得目标函数的**局部最小(最大)值**，因此也只能得到模型参数的**局部最优值**。\n",
    "\n",
    "因此，如果你想最简洁的介绍一个算法，列出这两个函数就行了。\n",
    "\n",
    "接下来，你会用**优化算法**去求取目标函数的最小(最大)值。**[随机]梯度{下降|上升}**算法就是一个**优化算法**。针对同一个**目标函数**，不同的**优化算法**会推导出不同的**训练规则**。我们后面还会讲其它的优化算法。\n",
    "\n",
    "其实在机器学习中，算法往往并不是关键，真正的关键之处在于选取特征。选取特征需要我们人类对问题的深刻理解，经验、以及思考。而**神经网络**算法的一个优势，就在于它能够自动学习到应该提取什么特征，从而使算法不再那么依赖人类，而这也是神经网络之所以吸引人的一个方面。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e98869e",
   "metadata": {},
   "source": [
    "## 23 款神经网络的设计和可视化工具\n",
    "\n",
    "[GitHub 地址](https://github.com/ashishpatel26/Tools-to-Design-or-Visualize-Architecture-of-Neural-Network)\n",
    "\n",
    "![nynqlv](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/nynqlv.jpg)\n",
    "\n",
    "![hk5ZFw](https://upiclw.oss-cn-beijing.aliyuncs.com/uPic/hk5ZFw.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4d1819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2960e171",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "274px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
