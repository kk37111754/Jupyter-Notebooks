{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>目录</h1>\n",
    "\n",
    "1. [生成式模型与判别式模型](#生成式模型与判别式模型)<br>\n",
    "   [1. 判别式模型](#1.-判别式模型)<br>\n",
    "   [2. 生成式模型](#2.-生成式模型)<br>\n",
    "   [3. 常见模型](#3.-常见模型)<br>\n",
    "   [4. 判别式模型与生成式模型比较](#4.-判别式模型与生成式模型比较)<br>\n",
    "\n",
    "2. [GAN的基本原理](#GAN的基本原理)<br>\n",
    "   [1. 基本原理](#1.-基本原理)<br>\n",
    "   [2. 优化目标与求解](#2.-优化目标与求解)<br>\n",
    "   [3. 如何训练](#3.-如何训练)<br>\n",
    "   [4. GAN的主要问题](#4.-GAN的主要问题)\n",
    "     \n",
    "3. [GAN的应用](#GAN的应用)<br>\n",
    "   [1. 数据生成](#1.-数据生成)<br>\n",
    "   [2. 风格迁移](#2.-风格迁移)<br>\n",
    "   [3. 超分辨重建](#3.-超分辨重建)<br>\n",
    " \n",
    "4. [参考文献](#参考文献)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成对抗网络（GANs）是当今计算机科学领域最有趣的想法之一。两个模型通过对抗过程同时训练。一个生成器（“艺术家”）学习创造看起来真实的图像，而判别器（“艺术评论家”）学习区分真假图像。\n",
    "\n",
    "训练过程中，生成器在生成逼真图像方面逐渐变强，而判别器在辨别这些图像的能力上逐渐变强。当判别器不再能够区分真实图片和伪造图片时，训练过程达到平衡。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 生成式模型与判别式模型\n",
    "\n",
    "正式说 GAN 之前我们先说一下判别式模型和生成式模型。\n",
    "\n",
    "## 1. 判别式模型\n",
    "\n",
    "判别式模型，即 Discriminative Model，又被称为条件概率模型，它估计的是条件概率分布(conditional distribution)， p(class|context) 。\n",
    "\n",
    "举个例子，我们给定(x,y)对，4个样本。(1,0), (1,0), (2,0), (2, 1)，p(y|x)是事件x发生时y的条件概率，它的计算如下：\n",
    "\n",
    "![判别式模型.](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161701639511270931617016394160.jpeg)\n",
    "\n",
    "\n",
    "## 2. 生成式模型\n",
    "\n",
    "即 Generative Model ，生成式模型 ，它估计的是联合概率分布（joint probability distribution），p(class,context)=p(class|context)*p(context) 。p(x,y)，即事件x与事件y同时发生的概率。同样以上面的样本为例，它的计算如下：\n",
    "\n",
    "![生成式模型](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161701649972744721617016499107.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 常见模型\n",
    "\n",
    "  \n",
    "常见的判别式模型有：\n",
    "\n",
    "* Logistic Regression（逻辑回归），\n",
    "* Linear Regression（线性回归），\n",
    "* SVM（支持向量机），\n",
    "* Traditional Neural Networks（遗传神经网络）， \n",
    "* Nearest Neighbor（最近邻），\n",
    "* CRF(Conditional random field/条件随机场) 等。\n",
    "\n",
    "常见的生成式模型有： \n",
    "\n",
    "* Naive Bayes（朴素贝叶斯），\n",
    "* Mixtures of Gaussians（高斯混合）， \n",
    "* HMMs（隐马尔可夫模型（Hidden Markov Model；缩写：HMM）或称作隐性马尔可夫模型），\n",
    "* Markov Random Fields（马尔卡夫随机场） 等。\n",
    "\n",
    "**得分点：（CV 面试中会出现）**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 判别式模型与生成式模型比较\n",
    "\n",
    "  \n",
    "判别式模型 ，优点是分类边界灵活 ，学习简单，性能较好 ；缺点是不能得到概率分布 。\n",
    "\n",
    "生成式模型 ，优点是收敛速度快，可学习分布，可应对隐变量 ；缺点是学习复杂 ，分类性能较差。\n",
    "\n",
    "![判别式模型与生成式模型比较](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161701959345131221617019590828.jpeg)\n",
    "\n",
    "\n",
    "上面是一个分类例子，可知判别式模型，有清晰的分界面，而生成式模型，有清晰的概率密度分布。生成式模型，可以转换为判别式模型，反之则不能。\n",
    "\n",
    "\n",
    "# GAN的基本原理\n",
    "\n",
    "GAN，即Generative adversarial net，它同时包含判别式模型和生成式模型，一个经典的网络结构如下：\n",
    "\n",
    "![GAN原理-1](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161701965974947691617019657505.jpeg)\n",
    "\n",
    "\n",
    "## 1. 基本原理\n",
    "\n",
    "![GAN原理-2](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161701969666050051617019694831.jpeg)\n",
    "\n",
    "GAN的原理很简单，它包括两个网络，一个生成网络，不断生成数据分布。一个判别网络，判断生成的数据是否为真实数据。上图是原理展示，黑色虚线是真实分布，绿色实线是生成模型的学习过程，蓝色虚线是判别模型的学习过程，两者相互对抗，共同学习到最优状态。\n",
    "\n",
    "## 2. 优化目标与求解\n",
    "\n",
    "下面是它的优化目标。\n",
    "\n",
    "![优化目标与求解-1](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161701976692835451617019765678.jpeg)\n",
    "\n",
    "D是判别器，它的学习目标，是最大化上面的式子，而G是生成器，它的学习目标，是最小化上面的式子。上面问题的求解，通过迭代求解D和G来完成。\n",
    "\n",
    "要求解上面的式子，等价于求解下面的式子。\n",
    "\n",
    "![优化目标与求解-2](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161701979026738271617019788693.jpeg)\n",
    "\n",
    "其中D(x)属于(0,1)，上式是alog(y) + blog(1−y)的形式，取得最大值的条件是D(x)=a/(a+b)，此时等价于下面式子。\n",
    "\n",
    "![优化目标与求解-3](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161701982451461061617019822677.jpeg)\n",
    "\n",
    "如果用KL散度来描述，上面的式子等于下面的式子。\n",
    "![优化目标与求解-4](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161701983716188751617019835893.jpeg)\n",
    "\n",
    "当且仅当pdata(x)=pg(x)时，取得极小值-log4，此时d=0.5，无法分辨真实样本和假样本。\n",
    "\n",
    "\n",
    "GAN从理论上，被证实存在全局最优解。\n",
    "## 3. 如何训练\n",
    "\n",
    "![如何训练](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161701985442714671617019850010.jpeg)\n",
    "\n",
    "直接从原始论文中截取伪代码了，可见，就是采用判别式模型和生成式模型分别循环依次迭代的方法，与CNN一样，使用梯度下降来优化。\n",
    "\n",
    "## 4. GAN的主要问题\n",
    "\n",
    "GAN从本质上来说，有与CNN不同的特点，因为GAN的训练是依次迭代D和G，如果判别器D学的不好，生成器G得不到正确反馈，就无法稳定学习。如果判别器D学的太好，整个loss迅速下降，G就无法继续学习。\n",
    "\n",
    "GAN的优化需要生成器和判别器达到纳什均衡，但是因为判别器D和生成器G是分别训练的，纳什平衡并不一定能达到，这是早期GAN难以训练的主要原因。另外，最初的损失函数也不是最优的，这些就留待我们的下篇再细讲吧，下面欣赏一下GAN的一些精彩的应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN的应用\n",
    "\n",
    "## 1. 数据生成\n",
    "\n",
    "- 从GAN到Conditional GAN\n",
    "\n",
    "GAN的生成式模型可以拟合真实分布，所以它可以用于伪造数据。DCGAN【3】是第一个用全卷积网络做数据生成的，下面是它的基本结构和生成的数据。\n",
    "\n",
    "![从GAN到Conditional_GAN-1](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161701988559799581617019883491.jpeg)\n",
    "\n",
    "![从GAN到Conditional_GAN-2](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161701990109180631617019898325.jpeg)\n",
    "\n",
    "输入100维的噪声，输出64×64的图像，从mnist的训练结果来看，还不错。\n",
    "\n",
    "\n",
    "但是它的问题是不能控制生成的数字是1还是9，所以后来有了CGAN【4】,即条件GAN，网络结构如下。\n",
    "![从GAN到Conditional_GAN-3](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161701991672354781617019914606.jpeg)\n",
    "\n",
    "它将标签信息encode为一个向量，串接到了D和G的输入进行训练，优化目标发生了改变。\n",
    "![从GAN到Conditional_GAN-4](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161701993271221401617019931969.jpeg)\n",
    "\n",
    "与cgan类似，infogan【5】将噪声z进行了拆解，一是不可压缩的噪声z，二是可解释的隐变量c，可以认为infogan就是无监督的cgan，这样能够约束c与生成数据之间的关系，控制一些属性，比如旋转等。\n",
    "![从GAN到Conditional_GAN-5](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161701994789648921617019945213.jpeg)\n",
    "\n",
    "条件GAN的出现，使得控制GAN的输出有了可能，出现了例如文本生成图像【6】的应用。\n",
    "![从GAN到Conditional_GAN-6](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161701996487339441617019960897.jpeg)\n",
    "\n",
    "\n",
    "- 金字塔GAN\n",
    "\n",
    "原始的GAN生成图的分辨率太小，无法实用，借鉴经典图像中的金字塔算法，LAPGAN【7】/StackedGAN8【8】各自提出类似的想法，下面是LAPGAN的结构。\n",
    "\n",
    "![金字塔GAN-1](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161701998150090281617019978853.jpeg)\n",
    "\n",
    "![金字塔GAN-2](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161701999740316921617019993697.jpeg)\n",
    "\n",
    "它有以下特点。\n",
    "\n",
    "(1)使用残差逼近，学习相对容易。\n",
    "\n",
    "(2)逐级独立训练提高了网络简单记忆输入样本的难度，减少了每一次 GAN 需要学习的内容，也就从而增大了 GAN 的学习能力和泛化能力。\n",
    "\n",
    "在这个基础上，nvidia-gan【9】生成了1024分辨率的图片，它的网络结构和生成结果如下。\n",
    "\n",
    "\n",
    "![金字塔GAN-3](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161702002445837381617020020813.jpeg)\n",
    "\n",
    "![金字塔GAN-4](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161702005098580601617020039658.jpeg)\n",
    "\n",
    "\n",
    "- cross domain学习\n",
    "\n",
    "cross domain的学习，提供了更丰富的数据生成应用。\n",
    "\n",
    "在传统的domain adaption中，我们需要学习或者训练一个domain adaptor，而这个domain adaptor需要用source domain和对应的target domain的训练图片来训练。coGAN【10】/align gan【11】可以在两个domain不存在对应样本的情况下学出一个联合分布，方法是每一个domain使用一个GAN，并且将高层的语义信息进行强制权值共享。\n",
    "\n",
    "![cross_domain学习-1](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161702006628777051617020063909.jpeg)\n",
    "\n",
    "![cross_domain学习-2](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161702009199447981617020077258.jpeg)\n",
    "\n",
    "在这样的基础上，有一些很有意义的应用。比如苹果simGAN【12】用于优化仿真数据的方案，此时生成器G的输入是合成图像，而不是随机向量，它完美学习到了人工合成图片（synthetic images）数据分布到真实图片（real images）数据分布的映射。\n",
    "\n",
    "![cross_domain学习-3](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161702014208687621617020138612.jpeg)\n",
    "\n",
    "下面是生成的结果，很有工程意义。\n",
    "\n",
    "![cross_domain学习-4](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161702016270935131617020155798.jpeg)\n",
    "\n",
    "- 一些很酷的应用\n",
    "\n",
    "下面再说一些很酷的应用，细节不再详述。\n",
    "\n",
    "creative-gan【13】，用于生成艺术风格的图片。\n",
    "\n",
    "![一些很酷的应用-1](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161702018203136741617020179180.jpeg)\n",
    "\n",
    "![一些很酷的应用-2](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161702216657863071617022165293.jpeg)\n",
    "\n",
    "DesignGan【14】，用于设计T恤。\n",
    "![一些很酷的应用-3](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161702022291233031617020221981.jpeg)\n",
    "![一些很酷的应用-4](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161702024079258301617020237215.jpeg)\n",
    "\n",
    "TP-GAN【15】，用于人脸正脸化。\n",
    "![一些很酷的应用-5](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161702026041752461617020255560.jpeg)\n",
    "\n",
    "![一些很酷的应用-6](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161702027913652451617020271988.jpeg)\n",
    "\n",
    "还有更多关于视频，音频的生成【16-19】\n",
    "\n",
    "\n",
    "## 2. 风格迁移\n",
    "\n",
    "文【20】实现了像素级别的风格转换，它的关键是提供了两个域中有相同数据的成对训练样本，本质上，是一个CGAN。\n",
    "\n",
    "![风格迁移-1](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161702029279981961617020291301.jpeg)\n",
    "\n",
    "\n",
    "cycle-gan【21】/dual-gan【22】则更胜一筹，不需要配对的数据集，可以实现源域和目标域的相互转换。\n",
    "\n",
    "\n",
    "![风格迁移-2](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161702031195743821617020310172.jpeg)\n",
    "\n",
    "\n",
    "pairedcycle【23】，将源域和目标域的相互转换用到化妆和去妆，很有趣的应用。\n",
    "![风格迁移-3](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161702032650891431617020324582.jpeg)\n",
    "\n",
    "文【24】学习了一个数据集到另一个数据集的迁移，可以用于迁移学习，如实现漫画风格。\n",
    "![风格迁移-4](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161702034685680851617020342536.jpeg)\n",
    "![风格迁移-5](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161702036651954121617020360316.jpeg)\n",
    "\n",
    "文【25】实现了动作的迁移。\n",
    "![风格迁移-6](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161702038624166821617020382974.jpeg)\n",
    "\n",
    "文【26】实现了年龄的仿真。\n",
    "![风格迁移-7](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161702040094421801617020396339.jpeg)\n",
    "\n",
    "文【27】提出了一种去雨的算法，很有实际意义。\n",
    "![风格迁移-8](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161702042381466161617020417924.jpeg)\n",
    "\n",
    "文【28】实现了卡通风格的转换。\n",
    "![风格迁移-9](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161702048515041881617020471266.jpeg)\n",
    "\n",
    "文【29】实现了字体风格的迁移。\n",
    "![风格迁移-10](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161702055046383121617020545729.jpeg)\n",
    "\n",
    "文【30】实现了去模糊。\n",
    "![风格迁移-11](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161702058580058641617020577412.jpeg)\n",
    "\n",
    "## 3. 超分辨重建\n",
    "\n",
    "srgan【31】是最早使用GAN做超分辨重建的应用，它将输入从随机噪声改为低分辨率的图片，使用了残差结构和perception loss，有很大的应用价值。\n",
    "\n",
    "![超分辨重建-1](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161702061843624781617020613166.jpeg)\n",
    "\n",
    "![超分辨重建-2](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161702064867114451617020640307.jpeg)\n",
    "\n",
    "超分辨重建可用于小脸的检测【32】，是个值得关注的方向。\n",
    "![超分辨重建-3](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/161702067412181791617020663581.jpeg)\n",
    "\n",
    "\n",
    "\n",
    "# 参考文献\n",
    "\n",
    "【1】Xue J H, Titterington D M. Comment on “On Discriminative vs. Generative Classifiers: A Comparison of Logistic Regression and Naive Bayes”[J]. Neural Processing Letters, 2008, 28(3):169.\n",
    "\n",
    "【2】Goodfellow I, Pouget-Abadie J, Mirza M, et al. Generative adversarial nets[C]//Advances in neural information processing systems. 2014: 2672-2680.\n",
    "\n",
    "【3】Radford A, Metz L, Chintala S. Unsupervised representation learning with deep convolutional generative adversarial networks[J]. arXiv preprint arXiv:1511.06434, 2015.\n",
    "\n",
    "【4】Mirza M, Osindero S. Conditional generative adversarial nets[J]. arXiv preprint arXiv:1411.1784, 2014.\n",
    "\n",
    "【5】Chen X, Duan Y, Houthooft R, et al. Infogan: Interpretable representation learning by information maximizing generative adversarial nets[C]//Advances in neural information processing systems. 2016: 2172-2180.\n",
    "\n",
    "【6】Reed S, Akata Z, Yan X, et al. Generative adversarial text to image synthesis[J]. arXiv preprint arXiv:1605.05396, 2016.\n",
    "\n",
    "【7】Denton E L, Chintala S, Fergus R. Deep generative image models using a￼ laplacian pyramid of adversarial networks[C]//Advances in neural information processing systems. 2015: 1486-1494.\n",
    "\n",
    "【8】Huang X, Li Y, Poursaeed O, et al. Stacked generative adversarial networks[C]//IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2017, 2(4).\n",
    "\n",
    "【9】Karras T, Aila T, Laine S, et al. Progressive growing of gans for improved quality, stability, and variation[J]. arXiv preprint arXiv:1710.10196, 2017.\n",
    "\n",
    "【10】Mao X, Li Q, Xie H. AlignGAN: Learning to align cross-domain images with conditional generative adversarial networks[J]. arXiv preprint arXiv:1707.01400, 2017.\n",
    "\n",
    "【11】Liu M Y, Tuzel O. Coupled generative adversarial networks[C]//Advances in neural information processing systems. 2016: 469-477.\n",
    "\n",
    "【12】Shrivastava A, Pfister T, Tuzel O, et al. Learning from simulated and unsupervised images through adversarial training[C]//The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2017, 3(4): 6.\n",
    "\n",
    "【13】Elgammal A, Liu B, Elhoseiny M, et al. CAN: Creative adversarial networks, generating\" art\" by learning about styles and deviating from style norms[J]. arXiv preprint arXiv:1706.07068, 2017.\n",
    "\n",
    "【14】Sbai O, Elhoseiny M, Bordes A, et al. DeSIGN: Design Inspiration from Generative Networks[J]. arXiv preprint arXiv:1804.00921, 2018.\n",
    "\n",
    "【15】Huang R, Zhang S, Li T, et al. Beyond face rotation: Global and local perception gan for photorealistic and identity preserving frontal view synthesis[J]. arXiv preprint arXiv:1704.04086, 2017.\n",
    "\n",
    "【16】Creswell A, Bharath A A. Adversarial training for sketch retrieval[C]//European Conference on Computer Vision. Springer, Cham, 2016: 798-809.\n",
    "\n",
    "【17】Tulyakov S, Liu M Y, Yang X, et al. Mocogan: Decomposing motion and content for video generation[J]. arXiv preprint arXiv:1707.04993, 2017.\n",
    "\n",
    "【18】Juvela L, Bollepalli B, Wang X, et al. Speech waveform synthesis from MFCC sequences with generative adversarial networks[J]. arXiv preprint arXiv:1804.00920, 2018.\n",
    "\n",
    "【19】Yu L, Zhang W, Wang J, et al. SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient[C]//AAAI. 2017: 2852-2858.\n",
    "\n",
    "【20】Isola P, Zhu J Y, Zhou T, et al. Image-to-Image Translation with Conditional Adversarial Networks[J]. 2016:5967-5976.\n",
    "\n",
    "【21】Zhu J Y, Park T, Isola P, et al. Unpaired image-to-image translation using cycle-consistent adversarial networks[J]. arXiv preprint, 2017.\n",
    "\n",
    "【22】Yi Z, Zhang H, Tan P, et al. Dualgan: Unsupervised dual learning for image-to-image translation[J]. arXiv preprint, 2017.\n",
    "\n",
    "【23】Chang H, Lu J, Yu F, et al. Pairedcyclegan: Asymmetric style transfer for applying and removing makeup[C]//2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2018.\n",
    "\n",
    "【24】Wei L, Zhang S, Gao W, et al. Person Transfer GAN to Bridge Domain Gap for Person Re-Identification[J]. arXiv preprint arXiv:1711.08565, 2017.\n",
    "\n",
    "【25】Joo D, Kim D, Kim J. Generating a Fusion Image: One's Identity and Another's Shape[J]. arXiv preprint arXiv:1804.07455, 2018.\n",
    "\n",
    "【26】Yang H, Huang D, Wang Y, et al. Learning face age progression: A pyramid architecture of gans[J]. arXiv preprint arXiv:1711.10352, 2017.\n",
    "\n",
    "【27】Qian R, Tan R T, Yang W, et al. Attentive Generative Adversarial Network for Raindrop Removal from A Single Image[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 2482-2491.\n",
    "\n",
    "【28】Chen Y, Lai Y K, Liu Y J. CartoonGAN: Generative Adversarial Networks for Photo Cartoonization[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 9465-9474.\n",
    "\n",
    "【29】Azadi S, Fisher M, Kim V, et al. Multi-content gan for few-shot font style transfer[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018, 11: 13.\n",
    "\n",
    "【30】Kupyn O, Budzan V, Mykhailych M, et al. DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks[J]. arXiv preprint arXiv:1711.07064, 2017.\n",
    "\n",
    "【31】Ledig C, Theis L, Huszár F, et al. Photo-realistic single image super-resolution using a generative adversarial network[J]. arXiv preprint, 2017.\n",
    "\n",
    "【32】Bai Y, Zhang Y, Ding M, et al. Finding tiny faces in the wild with generative adversarial network[J]. CVPR. IEEE, 2018.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "-- By：有三AI 团队\n",
    "\n",
    "聚焦于让大家能够系统性地完成AI各个领域所需的专业知识的学习，实现三人行必有AI，三人行必有我师的愿景。"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "tianchi_metadata": {
   "competitions": [],
   "datasets": [],
   "description": "",
   "notebookId": "348648",
   "source": "dsw"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "目录",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
