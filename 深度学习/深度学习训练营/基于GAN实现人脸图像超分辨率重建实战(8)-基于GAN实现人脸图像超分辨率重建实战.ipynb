{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "8ea99d61-e187-4174-94f4-511763783b42"
   },
   "source": [
    "<h1>目录</h1>\n",
    "\n",
    "1. [SRGAN超分辨模型](#SRGAN超分辨模型)<br>\n",
    "\n",
    "2. [SRGAN模型训练与测试](#SRGAN模型训练与测试)<br>\n",
    "   [1. 项目解读](#1.-项目解读)<br>\n",
    "   [2. 模型训练](#2.-模型训练)<br>\n",
    "   [3. 模型测试](#3.-模型测试)<br>\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "9f2c3a46-df43-4d64-8955-fb1ea34379cf"
   },
   "source": [
    "本项目需要在 GPU 环境下运行，点击本页面最右边 `<` 进行切换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "9dd5340d-ddbc-4f23-9d99-4cbc6b5472a7"
   },
   "source": [
    "# SRGAN超分辨模型\n",
    "\n",
    "随着生成对抗网络GAN的发展，生成器和判别器的对抗学习机制在图像生成任务中展现出很强大的学习能力。Twitter的研究者们使用ResNet作为生成器结构，使用VGG作为判别器结构，提出了SRGAN模型，这是本次实践课使用的模型，其结构示意图如下：\n",
    "\n",
    "<center>\n",
    "    <img style=\"border-radius: 0.3125em;\n",
    "    box-shadow: 0 8px 8px 0 rgba(34,36,38,.12),0 5px 10px 0 rgba(34,36,38,.08);\" \n",
    "    src=\"https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/162037471419435291620374710692.png\">\n",
    "    <br>\n",
    "    <div style=\"color:orange; border-bottom: 1px solid #d9d9d9;\n",
    "    display: inline-block;\n",
    "    color: #999;\n",
    "    padding: 2px;\">SRGAN模型</div>\n",
    "</center>\n",
    "\n",
    "\n",
    "生成器结构包含了若干个不改变特征分辨率的残差模块和多个基于亚像素卷积的后上采样模块。\n",
    "\n",
    "判别器结构则包含了若干个通道数不断增加的卷积层，每次特征通道数增加一倍时，特征分辨率降低为原来的一半。\n",
    "\n",
    "SRGAN模型的损失函数包括两部分，内容损失与对抗损失。\n",
    "\n",
    "$$\n",
    "l^{S R}=\\underbrace{\\underbrace{l_{\\mathrm{X}}^{S R}}_{\\text {content loss }}+\\underbrace{10^{-3} l_{G e n}^{S R}}_{\\text {adversarial loss }}}_{\\text {perceptual loss (for VGG based content losses) }}\n",
    "$$\n",
    "\n",
    "\n",
    "对抗损失就是标准的GAN损失，而内容损失则是基于VGG网络特征构建，它代替了之前SRCNN使用的MSE损失函数，如下：\n",
    "\n",
    "$$\n",
    "\\ell_{\\text {feat }}^{\\phi, j}(\\hat{y}, y)=\\frac{1}{C_{j} H_{j} W_{j}}\\left\\|\\phi_{j}(\\hat{y})-\\phi_{j}(y)\\right\\|_{2}^{2}\n",
    "$$\n",
    "\n",
    "SRGAN通过生成器和判别器的对抗学习取得了视觉感知上更好的重建结果。不过基于GAN的模型虽然可以取得好的超分结果，但是也往往容易放大噪声。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "3f25391a-8700-4576-8025-24bb23339d1c"
   },
   "source": [
    "# SRGAN模型训练与测试\n",
    "\n",
    "## 1. 项目解读\n",
    "\n",
    "下面我们首先来剖析整个项目的代码。\n",
    "\n",
    "### 1.1 数据集和基准模型\n",
    "\n",
    "首先我们来介绍使用的数据集和基准模型，大多数超分重建任务的数据集都是通过从高分辨率图像进行降采样获得，这里我们也采用这样的方案。数据集既可以选择ImageNet这样包含上百万图像的大型数据集，也可以选择模式足够丰富的小数据集，这里我们选择一个垂直领域的高清人脸数据集，CelebA-HQ。CelebA-HQ数据集发布于2019年，包含30000张包括不同属性的高清人脸图，其中图像大小均为1024×1024。\n",
    "\n",
    "数据集放置在项目根目录的 `dataset` 目录下，包括两个子文件夹，train 和 val。\n",
    "\n",
    "> 在项目开始之前需要加载数据集和预训练模型，加载方式如下图所示：\n",
    "**仅第一次使用时操作！！！**\n",
    "\n",
    "![](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/162039359855683351620393598194.jpeg)\n",
    "\n",
    "\n",
    "> 由于数据集较大，加载需要较长时间，加载完毕后，会有弹窗显示\n",
    "\n",
    "![](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/162038929056930491620389289872.jpeg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "b75a626c-a062-42a6-bae3-4e24f95c25ba"
   },
   "source": [
    "数据下载成功后，会得到两个 `zip` 文件，一个是数据集(`Face_SuperResolution_Dataset.zip`)，一个是预训练模型(`vgg16-397923af.zip`)\n",
    "\n",
    "![](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/162055927242481251620559272647.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "2be9310b-3ff3-4853-b42b-3f7662eefba3"
   },
   "source": [
    "**运行下方代码解压数据集，第一次使用时运行即可！！**，当显示 10 个 `.` 时，代表解压完成。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T06:26:48.112015Z",
     "start_time": "2021-05-08T06:26:47.868632Z"
    },
    "uuid": "8312411a-c255-4b27-b0fd-55f3364b53ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . . . . . . . "
     ]
    }
   ],
   "source": [
    "!unzip -o Face_SuperResolution_Dataset.zip | awk 'BEGIN {ORS=\" \"} {if(NR%3000==0)print \".\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "eacc3ca8-bddb-44ca-a4f5-846d786e17de"
   },
   "source": [
    "解压完成后会得到一个 `dataset` 文件夹，其文件结构如下\n",
    "\n",
    "```bash\n",
    "dataset\n",
    "    - train\n",
    "    - val\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "928f86ae-6661-4777-9acb-e7c7c69e07f8"
   },
   "source": [
    "**运行下方代码解压预训练模型，第一次使用时运行即可！！**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "uuid": "289eb4d1-97f6-432b-897d-a092486dc658"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  vgg16-397923af.zip\n",
      "  inflating: ./hub/checkpoints/vgg16-397923af.pth  \n"
     ]
    }
   ],
   "source": [
    "!mkdir checkpoints\n",
    "!unzip -o vgg16-397923af.zip -d ./checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "771a0703-3912-4f37-b5f0-5f72eed92645"
   },
   "source": [
    "### 1.2 数据集接口\n",
    "\n",
    "下面我们从高分辨率图进行采样得到低分辨率图，然后组成训练用的图像对，核心代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-09T09:08:22.206729Z",
     "start_time": "2021-05-09T09:08:19.957206Z"
    },
    "uuid": "627fbf1d-7eed-4fdf-a48d-535d58803d6b"
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision.transforms import Compose, RandomCrop, ToTensor, ToPILImage, CenterCrop, Resize\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(\n",
    "        filename.endswith(extension)\n",
    "        for extension in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'])\n",
    "\n",
    "\n",
    "# 基于上采样因子对裁剪尺寸进行调整，使其为upscale_factor的整数倍\n",
    "def calculate_valid_crop_size(crop_size, upscale_factor):\n",
    "    return crop_size - (crop_size % upscale_factor)\n",
    "\n",
    "\n",
    "# 训练集高分辨率图预处理函数\n",
    "def train_hr_transform(crop_size):\n",
    "    return Compose([\n",
    "        RandomCrop(crop_size),\n",
    "        ToTensor(),\n",
    "    ])\n",
    "\n",
    "\n",
    "# 训练集低分辨率图预处理函数\n",
    "def train_lr_transform(crop_size, upscale_factor):\n",
    "    return Compose([\n",
    "        ToPILImage(),\n",
    "        Resize(crop_size // upscale_factor, interpolation=Image.BICUBIC),\n",
    "        ToTensor()\n",
    "    ])\n",
    "\n",
    "\n",
    "def display_transform():\n",
    "    return Compose([ToPILImage(), Resize(400), CenterCrop(400), ToTensor()])\n",
    "\n",
    "\n",
    "# 训练数据集类\n",
    "class TrainDatasetFromFolder(Dataset):\n",
    "    def __init__(self, dataset_dir, crop_size, upscale_factor):\n",
    "        super(TrainDatasetFromFolder, self).__init__()\n",
    "        self.image_filenames = [\n",
    "            join(dataset_dir, x) for x in listdir(dataset_dir)\n",
    "            if is_image_file(x)\n",
    "        ]  # 获得所有图像\n",
    "        crop_size = calculate_valid_crop_size(crop_size,\n",
    "                                              upscale_factor)  # 获得裁剪尺寸\n",
    "        self.hr_transform = train_hr_transform(crop_size)  # 高分辨率图预处理函数\n",
    "        self.lr_transform = train_lr_transform(crop_size,\n",
    "                                               upscale_factor)  # 低分辨率图预处理函数\n",
    "\n",
    "    # 数据集迭代指针\n",
    "    def __getitem__(self, index):\n",
    "        hr_image = self.hr_transform(Image.open(\n",
    "            self.image_filenames[index]))  # 随机裁剪获得高分辨率图\n",
    "        lr_image = self.lr_transform(hr_image)  # 获得低分辨率图\n",
    "        return lr_image, hr_image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "\n",
    "# 验证数据集类\n",
    "class ValDatasetFromFolder(Dataset):\n",
    "    def __init__(self, dataset_dir, upscale_factor):\n",
    "        super(ValDatasetFromFolder, self).__init__()\n",
    "        self.upscale_factor = upscale_factor\n",
    "        self.image_filenames = [\n",
    "            join(dataset_dir, x) for x in listdir(dataset_dir)\n",
    "            if is_image_file(x)\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        hr_image = Image.open(self.image_filenames[index])\n",
    "\n",
    "        # 获得图像窄边获得裁剪尺寸\n",
    "        w, h = hr_image.size\n",
    "        crop_size = calculate_valid_crop_size(min(w, h), self.upscale_factor)\n",
    "        lr_scale = Resize(crop_size // self.upscale_factor,\n",
    "                          interpolation=Image.BICUBIC)\n",
    "        hr_scale = Resize(crop_size, interpolation=Image.BICUBIC)\n",
    "        hr_image = CenterCrop(crop_size)(hr_image)  # 中心裁剪获得高分辨率图\n",
    "        lr_image = lr_scale(hr_image)  # 获得低分辨率图\n",
    "        hr_restore_img = hr_scale(lr_image)\n",
    "        return ToTensor()(lr_image), ToTensor()(hr_restore_img), ToTensor()(\n",
    "            hr_image)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "\n",
    "class TestDatasetFromFolder(Dataset):\n",
    "    def __init__(self, dataset_dir, upscale_factor):\n",
    "        super(TestDatasetFromFolder, self).__init__()\n",
    "        self.lr_path = dataset_dir + '/SRF_' + str(upscale_factor) + '/data/'\n",
    "        self.hr_path = dataset_dir + '/SRF_' + str(upscale_factor) + '/target/'\n",
    "        self.upscale_factor = upscale_factor\n",
    "        self.lr_filenames = [\n",
    "            join(self.lr_path, x) for x in listdir(self.lr_path)\n",
    "            if is_image_file(x)\n",
    "        ]\n",
    "        self.hr_filenames = [\n",
    "            join(self.hr_path, x) for x in listdir(self.hr_path)\n",
    "            if is_image_file(x)\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.lr_filenames[index].split('/')[-1]\n",
    "        lr_image = Image.open(self.lr_filenames[index])\n",
    "        w, h = lr_image.size\n",
    "        hr_image = Image.open(self.hr_filenames[index])\n",
    "        hr_scale = Resize((self.upscale_factor * h, self.upscale_factor * w),\n",
    "                          interpolation=Image.BICUBIC)\n",
    "        hr_restore_img = hr_scale(lr_image)\n",
    "        return image_name, ToTensor()(lr_image), ToTensor()(\n",
    "            hr_restore_img), ToTensor()(hr_image)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "57a14cf5-7ceb-4693-b956-086e056cc65e"
   },
   "source": [
    "从上述代码可以看出，包含了两个预处理函数接口，分别是train_hr_transform，train_lr_transform。train_hr_transform包含的操作主要是随机裁剪，而train_lr_transform包含的操作主要是缩放。\n",
    "\n",
    "另外还有一个函数calculate_valid_crop_size，对于训练集来说，它用于当配置的图像尺寸crop_size不能整除上采样因子upscale_factor时对crop_size进行调整，我们在使用的时候应该避免这一点，即配置crop_size让它等于upscale_factor的整数倍。对于验证集，图像的窄边min(w, h)会被用于crop_size的初始化，所以该函数的作用是当图像的窄边不能整除上采样因子upscale_factor时对crop_size进行调整。\n",
    "\n",
    "训练集类TrainDatasetFromFolder包含了若干操作，它使用train_hr_transform从原图像中随机裁剪大小为裁剪尺寸的正方形的图像，使用train_lr_transform获得对应的低分辨率图。而验证集类ValDatasetFromFolder则将图像按照调整后的crop_size进行中心裁剪，然后使用train_lr_transform获得对应的低分辨率图。\n",
    "\n",
    "在这里我们只使用了随机裁剪作为训练时的数据增强操作，实际训练工程项目时，应该根据需要添加多种数据增强操作才能获得泛化能力更好的模型。\n",
    "\n",
    "\n",
    "### 1.3 生成器\n",
    "\n",
    "生成器是一个基于残差模块的上采样模型，它的定义包括残差模块，上采样模块以及主干模型，如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-09T09:08:25.545144Z",
     "start_time": "2021-05-09T09:08:25.522146Z"
    },
    "uuid": "9f9bb270-dcd1-4596-98cd-6c496d24f225"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# 生成模型\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, scale_factor):\n",
    "        upsample_block_num = int(math.log(scale_factor, 2))\n",
    "\n",
    "        super(Generator, self).__init__()\n",
    "        # 第一个卷积层，卷积核大小为9×9，输入通道数为3，输出通道数为64\n",
    "        self.block1 = nn.Sequential(nn.Conv2d(3, 64, kernel_size=9, padding=4),\n",
    "                                    nn.PReLU())\n",
    "        # 6个残差模块\n",
    "        self.block2 = ResidualBlock(64)\n",
    "        self.block3 = ResidualBlock(64)\n",
    "        self.block4 = ResidualBlock(64)\n",
    "        self.block5 = ResidualBlock(64)\n",
    "        self.block6 = ResidualBlock(64)\n",
    "        self.block7 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64))\n",
    "        # upsample_block_num个上采样模块，每一个上采样模块恢复2倍的上采样倍率\n",
    "        block8 = [UpsampleBLock(64, 2) for _ in range(upsample_block_num)]\n",
    "        # 最后一个卷积层，卷积核大小为9×9，输入通道数为64，输出通道数为3\n",
    "        block8.append(nn.Conv2d(64, 3, kernel_size=9, padding=4))\n",
    "        self.block8 = nn.Sequential(*block8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        block1 = self.block1(x)\n",
    "        block2 = self.block2(block1)\n",
    "        block3 = self.block3(block2)\n",
    "        block4 = self.block4(block3)\n",
    "        block5 = self.block5(block4)\n",
    "        block6 = self.block6(block5)\n",
    "        block7 = self.block7(block6)\n",
    "        block8 = self.block8(block1 + block7)\n",
    "\n",
    "        return (torch.tanh(block8) + 1) / 2\n",
    "\n",
    "\n",
    "# 残差模块\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        # 两个卷积层，卷积核大小为3×3，通道数不变\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.prelu = nn.PReLU()\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.conv1(x)\n",
    "        residual = self.bn1(residual)\n",
    "        residual = self.prelu(residual)\n",
    "        residual = self.conv2(residual)\n",
    "        residual = self.bn2(residual)\n",
    "\n",
    "        return x + residual\n",
    "\n",
    "\n",
    "# 上采样模块，每一个恢复分辨率为2\n",
    "class UpsampleBLock(nn.Module):\n",
    "    def __init__(self, in_channels, up_scale):\n",
    "        super(UpsampleBLock, self).__init__()\n",
    "        # 卷积层，输入通道数为in_channels，输出通道数为in_channels * up_scale ** 2\n",
    "        self.conv = nn.Conv2d(in_channels,\n",
    "                              in_channels * up_scale**2,\n",
    "                              kernel_size=3,\n",
    "                              padding=1)\n",
    "        # PixelShuffle上采样层，来自于后上采样结构\n",
    "        self.pixel_shuffle = nn.PixelShuffle(up_scale)\n",
    "        self.prelu = nn.PReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pixel_shuffle(x)\n",
    "        x = self.prelu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "3ceb5059-f5d4-44a2-938b-fd7658be0394"
   },
   "source": [
    "在上述的生成器定义中，调用了nn.PixelShuffle模块来实现上采样，它的具体原理在上节基于亚像素卷积的后上采样ESPCN模型中有详细介绍。\n",
    "\n",
    "### 1.4 判别器\n",
    "\n",
    "判别器是一个普通的类似于VGG的CNN模型，完整定义如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-09T09:08:27.048581Z",
     "start_time": "2021-05-09T09:08:27.021584Z"
    },
    "uuid": "02cd542d-9452-4535-bfc9-8050ea817acf"
   },
   "outputs": [],
   "source": [
    "# 残差模块\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # 第1个卷积层，卷积核大小为3×3，输入通道数为3，输出通道数为64\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # 第2个卷积层，卷积核大小为3×3，输入通道数为64，输出通道数为64\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # 第3个卷积层，卷积核大小为3×3，输入通道数为64，输出通道数为128\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # 第4个卷积层，卷积核大小为3×3，输入通道数为128，输出通道数为128\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # 第5个卷积层，卷积核大小为3×3，输入通道数为128，输出通道数为256\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # 第6个卷积层，卷积核大小为3×3，输入通道数为256，输出通道数为256\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # 第7个卷积层，卷积核大小为3×3，输入通道数为256，输出通道数为512\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # 第8个卷积层，卷积核大小为3×3，输入通道数为512，输出通道数为512\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # 全局池化层\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            # 两个全连接层，使用卷积实现\n",
    "            nn.Conv2d(512, 1024, kernel_size=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(1024, 1, kernel_size=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        return torch.sigmoid(self.net(x).view(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "7b2ee5bc-44b8-4e74-87d0-16c2ca29f40a"
   },
   "source": [
    "### 1.5 损失定义\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-09T09:08:39.821223Z",
     "start_time": "2021-05-09T09:08:28.486064Z"
    },
    "uuid": "c79159d2-e096-46ac-8f9b-bf0dfb8a04e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeneratorLoss(\n",
      "  (loss_network): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (mse_loss): MSELoss()\n",
      "  (tv_loss): TVLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models.vgg import vgg16\n",
    "import os\n",
    "os.environ['TORCH_HOME'] = './'\n",
    "\n",
    "\n",
    "# 生成器损失定义\n",
    "class GeneratorLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GeneratorLoss, self).__init__()\n",
    "        vgg = vgg16(pretrained=True)\n",
    "        loss_network = nn.Sequential(*list(vgg.features)[:31]).eval()\n",
    "        for param in loss_network.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.loss_network = loss_network\n",
    "        self.mse_loss = nn.MSELoss()  # MSE损失\n",
    "        self.tv_loss = TVLoss()  # TV平滑损失\n",
    "\n",
    "    def forward(self, out_labels, out_images, target_images):\n",
    "        # 对抗损失\n",
    "        adversarial_loss = torch.mean(1 - out_labels)\n",
    "        # 感知损失\n",
    "        perception_loss = self.mse_loss(self.loss_network(out_images),\n",
    "                                        self.loss_network(target_images))\n",
    "        # 图像MSE损失\n",
    "        image_loss = self.mse_loss(out_images, target_images)\n",
    "        # TV平滑损失\n",
    "        tv_loss = self.tv_loss(out_images)\n",
    "        return image_loss + 0.001 * adversarial_loss + 0.006 * perception_loss + 2e-8 * tv_loss\n",
    "\n",
    "\n",
    "# TV平滑损失\n",
    "class TVLoss(nn.Module):\n",
    "    def __init__(self, tv_loss_weight=1):\n",
    "        super(TVLoss, self).__init__()\n",
    "        self.tv_loss_weight = tv_loss_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size()[0]\n",
    "        h_x = x.size()[2]\n",
    "        w_x = x.size()[3]\n",
    "        count_h = self.tensor_size(x[:, :, 1:, :])\n",
    "        count_w = self.tensor_size(x[:, :, :, 1:])\n",
    "        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()\n",
    "        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()\n",
    "        return self.tv_loss_weight * 2 * (h_tv / count_h +\n",
    "                                          w_tv / count_w) / batch_size\n",
    "\n",
    "    @staticmethod\n",
    "    def tensor_size(t):\n",
    "        return t.size()[1] * t.size()[2] * t.size()[3]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    g_loss = GeneratorLoss()\n",
    "    print(g_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "0ef1c27f-e267-4a99-9da4-aabe834c3eaa"
   },
   "source": [
    "生成器损失总共包含4部分，分别是对抗网络损失，逐像素的图像MSE损失，基于VGG模型的感知损失，用于约束图像平滑的TV平滑损失。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "6ec71384-953d-4f67-8c22-de511eb37f1d"
   },
   "source": [
    "## 2. 模型训练\n",
    "\n",
    "接下来我们来解读模型的核心训练代码，查看模型训练的结果。训练代码除了模型和损失定义，还需要完成优化器定义，训练和验证指标变量的存储，核心代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-09T09:08:39.884233Z",
     "start_time": "2021-05-09T09:08:39.858229Z"
    },
    "uuid": "472a2e73-a94f-4fa3-98d7-e436dae43265"
   },
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2)) for x in range(window_size)])\n",
    "    return gauss / gauss.sum()\n",
    "\n",
    "\n",
    "def create_window(window_size, channel):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
    "    return window\n",
    "\n",
    "\n",
    "def _ssim(img1, img2, window, window_size, channel, size_average=True):\n",
    "    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2\n",
    "\n",
    "    C1 = 0.01 ** 2\n",
    "    C2 = 0.03 ** 2\n",
    "\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    if size_average:\n",
    "        return ssim_map.mean()\n",
    "    else:\n",
    "        return ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "\n",
    "class SSIM(torch.nn.Module):\n",
    "    def __init__(self, window_size=11, size_average=True):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "        self.channel = 1\n",
    "        self.window = create_window(window_size, self.channel)\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        (_, channel, _, _) = img1.size()\n",
    "\n",
    "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
    "            window = self.window\n",
    "        else:\n",
    "            window = create_window(self.window_size, channel)\n",
    "\n",
    "            if img1.is_cuda:\n",
    "                window = window.cuda(img1.get_device())\n",
    "            window = window.type_as(img1)\n",
    "\n",
    "            self.window = window\n",
    "            self.channel = channel\n",
    "\n",
    "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
    "\n",
    "\n",
    "def ssim(img1, img2, window_size=11, size_average=True):\n",
    "    (_, channel, _, _) = img1.size()\n",
    "    window = create_window(window_size, channel)\n",
    "\n",
    "    if img1.is_cuda:\n",
    "        window = window.cuda(img1.get_device())\n",
    "    window = window.type_as(img1)\n",
    "\n",
    "    return _ssim(img1, img2, window, window_size, channel, size_average)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "d9e0e39b-7b37-486e-9628-fe6aac41d59a"
   },
   "source": [
    "创建一些文件夹，首次使用时运行！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "uuid": "45bba781-1f9f-4ddc-8e3e-7a93413d69ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘training_results’: File exists\n",
      "mkdir: cannot create directory ‘epochs’: File exists\n",
      "mkdir: cannot create directory ‘statistics’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir training_results\n",
    "!mkdir epochs\n",
    "!mkdir statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "249fda45-7387-49b2-89db-5d8b2475ccce"
   },
   "source": [
    "**注意：由于阿里云平台 GPU 资源受限，本项目仅使用少量数据集进行训练**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-09T09:08:33.189Z"
    },
    "uuid": "d42a049a-b504-4535-89a2-eec0b85ff0ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1/20] Loss_D: 0.9206 Loss_G: 0.0191 D(x): 0.3966 D(G(z)): 0.2900: 100%|██████████| 44/44 [00:31<00:00,  1.40it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 45.18it/s]\n",
      "[2/20] Loss_D: 0.9882 Loss_G: 0.0099 D(x): 0.3624 D(G(z)): 0.3444: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 45.75it/s]\n",
      "[3/20] Loss_D: 0.9785 Loss_G: 0.0087 D(x): 0.3593 D(G(z)): 0.3436: 100%|██████████| 44/44 [00:31<00:00,  1.40it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 45.57it/s]\n",
      "[4/20] Loss_D: 1.0011 Loss_G: 0.0070 D(x): 0.5323 D(G(z)): 0.5201: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 45.72it/s]\n",
      "[5/20] Loss_D: 0.9912 Loss_G: 0.0071 D(x): 0.3812 D(G(z)): 0.3706: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 45.98it/s]\n",
      "[6/20] Loss_D: 0.9738 Loss_G: 0.0072 D(x): 0.4276 D(G(z)): 0.3970: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 45.68it/s]\n",
      "[7/20] Loss_D: 1.0016 Loss_G: 0.0066 D(x): 0.1955 D(G(z)): 0.1928: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 45.08it/s]\n",
      "[8/20] Loss_D: 1.0028 Loss_G: 0.0064 D(x): 0.1531 D(G(z)): 0.1508: 100%|██████████| 44/44 [00:31<00:00,  1.40it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 45.72it/s]\n",
      "[9/20] Loss_D: 1.0018 Loss_G: 0.0066 D(x): 0.0594 D(G(z)): 0.0610: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 45.19it/s]\n",
      "[10/20] Loss_D: 0.9963 Loss_G: 0.0061 D(x): 0.0795 D(G(z)): 0.0764: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 45.63it/s]\n",
      "[11/20] Loss_D: 1.0042 Loss_G: 0.0061 D(x): 0.1649 D(G(z)): 0.1674: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 45.91it/s]\n",
      "[12/20] Loss_D: 0.9918 Loss_G: 0.0058 D(x): 0.2955 D(G(z)): 0.2907: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 45.34it/s]\n",
      "[13/20] Loss_D: 1.0028 Loss_G: 0.0056 D(x): 0.2586 D(G(z)): 0.2455: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 45.59it/s]\n",
      "[14/20] Loss_D: 1.0006 Loss_G: 0.0057 D(x): 0.1642 D(G(z)): 0.1645: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 45.12it/s]\n",
      "[15/20] Loss_D: 0.9968 Loss_G: 0.0057 D(x): 0.2240 D(G(z)): 0.2179: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 45.46it/s]\n",
      "[16/20] Loss_D: 1.0059 Loss_G: 0.0055 D(x): 0.1927 D(G(z)): 0.1983: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 45.51it/s]\n",
      "[17/20] Loss_D: 0.9990 Loss_G: 0.0057 D(x): 0.2181 D(G(z)): 0.2166: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 45.56it/s]\n",
      "[18/20] Loss_D: 0.9990 Loss_G: 0.0051 D(x): 0.2205 D(G(z)): 0.2180: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 45.73it/s]\n",
      "[19/20] Loss_D: 1.0063 Loss_G: 0.0051 D(x): 0.2115 D(G(z)): 0.2121: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 45.92it/s]\n",
      "[20/20] Loss_D: 0.9974 Loss_G: 0.0050 D(x): 0.1125 D(G(z)): 0.1072: 100%|██████████| 44/44 [00:31<00:00,  1.41it/s]\n",
      "100%|██████████| 85/85 [00:01<00:00, 44.92it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from math import log10\n",
    "\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.utils as utils\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    CROP_SIZE = 240 #opt.crop_size   ## 裁剪尺寸，即训练尺度\n",
    "    UPSCALE_FACTOR = 4#opt.upscale_factor  ## 超分上采样倍率\n",
    "    NUM_EPOCHS = 20  #opt.num_epochs  ## 迭代epoch次数\n",
    "    \n",
    "    ## 获取训练集/验证集\n",
    "    train_set = TrainDatasetFromFolder('dataset/train', crop_size=CROP_SIZE, upscale_factor=UPSCALE_FACTOR)\n",
    "    val_set = ValDatasetFromFolder('dataset/val', upscale_factor=UPSCALE_FACTOR)\n",
    "    train_loader = DataLoader(dataset=train_set, num_workers=4, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(dataset=val_set, num_workers=4, batch_size=1, shuffle=False)\n",
    "  \n",
    "    netG = Generator(UPSCALE_FACTOR) ##生成器定义\n",
    "    netD = Discriminator() ##判别器定义\n",
    "    generator_criterion = GeneratorLoss() ##生成器优化目标\n",
    "    \n",
    "    ## 是否使用GPU\n",
    "    if torch.cuda.is_available():\n",
    "        netG.cuda()\n",
    "        netD.cuda()\n",
    "        generator_criterion.cuda()\n",
    "    \n",
    "    ##生成器和判别器优化器\n",
    "    optimizerG = optim.Adam(netG.parameters())\n",
    "    optimizerD = optim.Adam(netD.parameters())\n",
    "    \n",
    "    results = {'d_loss': [], 'g_loss': [], 'd_score': [], 'g_score': [], 'psnr': [], 'ssim': []}\n",
    "    ## epoch迭代\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        train_bar = tqdm(train_loader)\n",
    "        running_results = {'batch_sizes': 0, 'd_loss': 0, 'g_loss': 0, 'd_score': 0, 'g_score': 0} ##结果变量\n",
    "    \n",
    "        netG.train() ##生成器训练\n",
    "        netD.train() ##判别器训练\n",
    "\n",
    "        ## 每一个epoch的数据迭代\n",
    "        for data, target in train_bar:\n",
    "            g_update_first = True\n",
    "            batch_size = data.size(0)\n",
    "            running_results['batch_sizes'] += batch_size\n",
    "    \n",
    "            ## 优化判别器，最大化D(x)-1-D(G(z))\n",
    "            real_img = Variable(target)\n",
    "            if torch.cuda.is_available():\n",
    "                real_img = real_img.cuda()\n",
    "            z = Variable(data)\n",
    "            if torch.cuda.is_available():\n",
    "                z = z.cuda()\n",
    "            fake_img = netG(z) ##获取生成结果\n",
    "            netD.zero_grad()\n",
    "            real_out = netD(real_img).mean()\n",
    "            fake_out = netD(fake_img).mean()\n",
    "            d_loss = 1 - real_out + fake_out\n",
    "            d_loss.backward(retain_graph=True)\n",
    "            optimizerD.step() ##优化判别器\n",
    "    \n",
    "            ## 优化生成器 最小化1-D(G(z)) + Perception Loss + Image Loss + TV Loss\n",
    "            netG.zero_grad()\n",
    "            g_loss = generator_criterion(fake_out, fake_img, real_img)\n",
    "            g_loss.backward()\n",
    "            \n",
    "            fake_img = netG(z)\n",
    "            fake_out = netD(fake_img).mean()\n",
    "            optimizerG.step()\n",
    "\n",
    "            # 记录当前损失\n",
    "            running_results['g_loss'] += g_loss.item() * batch_size\n",
    "            running_results['d_loss'] += d_loss.item() * batch_size\n",
    "            running_results['d_score'] += real_out.item() * batch_size\n",
    "            running_results['g_score'] += fake_out.item() * batch_size\n",
    "            train_bar.set_description(desc='[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f' % (\n",
    "                epoch, NUM_EPOCHS, running_results['d_loss'] / running_results['batch_sizes'],\n",
    "                running_results['g_loss'] / running_results['batch_sizes'],\n",
    "                running_results['d_score'] / running_results['batch_sizes'],\n",
    "                running_results['g_score'] / running_results['batch_sizes']))\n",
    "        \n",
    "        ## 对验证集进行验证\n",
    "        netG.eval() ## 设置验证模式\n",
    "        out_path = 'training_results/SRF_' + str(UPSCALE_FACTOR) + '/'\n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "        \n",
    "        ## 计算验证集相关指标\n",
    "        with torch.no_grad():\n",
    "            val_bar = tqdm(val_loader)\n",
    "            valing_results = {'mse': 0, 'ssims': 0, 'psnr': 0, 'ssim': 0, 'batch_sizes': 0}\n",
    "            val_images = []\n",
    "            for val_lr, val_hr_restore, val_hr in val_bar:\n",
    "                batch_size = val_lr.size(0)\n",
    "                valing_results['batch_sizes'] += batch_size\n",
    "                lr = val_lr ##低分辨率真值图\n",
    "                hr = val_hr ##高分辨率真值图\n",
    "                if torch.cuda.is_available():\n",
    "                    lr = lr.cuda() \n",
    "                    hr = hr.cuda()\n",
    "                sr = netG(lr) ##超分重建结果\n",
    "            \n",
    "                batch_mse = ((sr - hr) ** 2).data.mean() ##计算MSE指标\n",
    "                valing_results['mse'] += batch_mse * batch_size\n",
    "                valing_results['psnr'] = 10 * log10(1 / (valing_results['mse'] / valing_results['batch_sizes'])) ##计算PSNR指标\n",
    "                batch_ssim = ssim(sr, hr).item() ##计算SSIM指标\n",
    "                valing_results['ssims'] += batch_ssim * batch_size\n",
    "                valing_results['ssim'] = valing_results['ssims'] / valing_results['batch_sizes']\n",
    "        ## 存储模型参数\n",
    "        torch.save(netG.state_dict(), 'epochs/netG_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))\n",
    "        torch.save(netD.state_dict(), 'epochs/netD_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))\n",
    "        ## 记录训练集损失以及验证集的psnr,ssim等指标 \\scores\\psnr\\ssim\n",
    "        results['d_loss'].append(running_results['d_loss'] / running_results['batch_sizes'])\n",
    "        results['g_loss'].append(running_results['g_loss'] / running_results['batch_sizes'])\n",
    "        results['d_score'].append(running_results['d_score'] / running_results['batch_sizes'])\n",
    "        results['g_score'].append(running_results['g_score'] / running_results['batch_sizes'])\n",
    "        results['psnr'].append(valing_results['psnr'])\n",
    "        results['ssim'].append(valing_results['ssim'])\n",
    "        \n",
    "        ## 存储结果到本地文件\n",
    "        if epoch % 10 == 0 and epoch != 0:\n",
    "            out_path = 'statistics/'\n",
    "            data_frame = pd.DataFrame(\n",
    "                data={'Loss_D': results['d_loss'], 'Loss_G': results['g_loss'], 'Score_D': results['d_score'],\n",
    "                      'Score_G': results['g_score'], 'PSNR': results['psnr'], 'SSIM': results['ssim']},\n",
    "                index=range(1, epoch + 1))\n",
    "            data_frame.to_csv(out_path + 'srf_' + str(UPSCALE_FACTOR) + '_train_results.csv', index_label='Epoch')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "7ef78be0-9a3c-4064-b2f1-e197bb76370a"
   },
   "source": [
    "从上述代码可以看出，训练时采用的crop_size为240×240，批处理大小为16，使用的优化器为Adam，Adam采用了默认的优化参数。\n",
    "\n",
    "损失等相关数据将生成在 `statistics` 文件夹下。\n",
    "\n",
    "\n",
    "上采样倍率为4的模型训练结果如下：\n",
    "\n",
    "<center>\n",
    "    <img style=\"border-radius: 0.3125em;\n",
    "    box-shadow: 0 8px 8px 0 rgba(34,36,38,.12),0 5px 10px 0 rgba(34,36,38,.08);\" \n",
    "    src=\"https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/162037474187417431620374740103.png\">\n",
    "    <br>\n",
    "    <div style=\"color:orange; border-bottom: 1px solid #d9d9d9;\n",
    "    display: inline-block;\n",
    "    color: #999;\n",
    "    padding: 2px;\">4倍上采样的PSNR和SSIM曲线</div>\n",
    "</center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "48b2f9fb-c96a-44c5-aa70-b4903d8b980a"
   },
   "source": [
    "## 3. 模型测试\n",
    "\n",
    "接下来我们进行模型的测试。\n",
    "\n",
    "### 3.1 测试代码\n",
    "\n",
    "首先解读测试代码，需要完成模型的载入，图像预处理和结果存储，完整代码如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "uuid": "28281459-a3c7-40c3-8ee6-663780387e6c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "\n",
    "\n",
    "UPSCALE_FACTOR = 4 ##上采样倍率\n",
    "TEST_MODE = True ## 使用GPU进行测试\n",
    "\n",
    "IMAGE_NAME = \"./dataset/val/10879.jpg\"  # 测试图片路径\n",
    "\n",
    "MODEL_NAME = './epochs/netG_epoch_4_20.pth' ##模型路径\n",
    "model = Generator(UPSCALE_FACTOR).eval() ##设置验证模式\n",
    "if TEST_MODE:\n",
    "    model.cuda()\n",
    "    model.load_state_dict(torch.load(MODEL_NAME))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(MODEL_NAME, map_location=lambda storage, loc: storage))\n",
    "\n",
    "image = Image.open(IMAGE_NAME) ##读取图片\n",
    "image = Variable(ToTensor()(image), volatile=True).unsqueeze(0) ##图像预处理\n",
    "if TEST_MODE:\n",
    "    image = image.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    RESULT_NAME = \"out_srf_\" + str(UPSCALE_FACTOR) + \"_\" + IMAGE_NAME.split(\"/\")[-1]\n",
    "    out = model(image)\n",
    "    out_img = ToPILImage()(out[0].data.cpu())\n",
    "    out_img.save(RESULT_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "90f79f67-0186-433b-9265-0a2e45cabe4f"
   },
   "source": [
    "预测结果将在本级目录生成，以 `out_srf_` 开头"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "db4e0587-6279-4fce-acc8-ff408e5585e1"
   },
   "source": [
    "### 3.2 重建结果\n",
    "\n",
    "下图展示了若干图片的超分辨结果。\n",
    "\n",
    "第一行为使用双线性插值进行上采样的结果, 第二行为4倍超分结果，第三行为原始大图。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "8e8dc34f-7818-4b52-b129-5c20fbc6a03d"
   },
   "source": [
    "![](https://tianchi-public.oss-cn-hangzhou.aliyuncs.com/public/files/forum/162039457398393701620394536576.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "uuid": "7e71d68b-ab5d-4a44-ba84-2b3abf40d817"
   },
   "source": [
    "本次我们对SRGAN模型进行了实践，使用高清人脸数据集进行训练，对低分辨率的人脸图像进行了超分重建，验证了SRGAN模型的有效性，不过该模型仍然有较大的改进空间，它需要使用成对数据集进行训练，而训练时低分辨率图片的模式产生过于简单，无法对复杂的退化类型完成重建。\n",
    "\n",
    "当要对退化类型更加复杂的图像进行超分辨重建时，模型训练时也应该采取多种对应的数据增强方法，包括但不限于对比度增强，各类噪声污染，JPEG压缩失真等操作，这些就留给读者去做更多的实验。\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "tianchi_metadata": {
   "competitions": [],
   "datasets": [
    {
     "id": "99975",
     "title": "Face_SuperResolution_Dataset"
    }
   ],
   "description": "",
   "notebookId": "201612",
   "source": "dsw"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
